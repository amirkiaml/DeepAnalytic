{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f36413d",
   "metadata": {},
   "source": [
    "# Creating the Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bcb65d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the column names\n",
    "column_names = 'IndexName NumArticles Splitter ChunkSize EmbeddingModel Query QueryType NumQueriesGenerated NumDocsPerQuery RerankCritique OrigQuery GenQueries DocsPerQuery'.split()\n",
    "\n",
    "# Create an empty DataFrame with specified column names\n",
    "test = pd.DataFrame(columns=column_names)\n",
    "\n",
    "# Optionally, specify the types for each column if necessary\n",
    "# Example types could be:\n",
    "column_types = {\n",
    "    'IndexName': 'string',  # String for textual data\n",
    "    'NumArticles': 'int',   # Integer for numerical counts\n",
    "    'Splitter': 'string',   # String for categorical text\n",
    "    'ChunkSize': 'int',     # Integer for numerical counts\n",
    "    'EmbeddingModel': 'string',  # String for textual data\n",
    "    'Query': 'string',\n",
    "    'QueryType': 'string',  # 'S' for Simple, 'C' for Complex, hence string\n",
    "    'NumQueriesGenerated': 'int',    # Integer for numerical counts\n",
    "    'NumDocsPerQuery': 'int',  # Integer for numerical counts\n",
    "    'RerankCritique': 'string',  # 'R' for Rerank, 'C' for Critique, 'N' for Neither, hence string\n",
    "    'OrigQuery': 'string',  # String for textual data\n",
    "    'GenQueries': 'object', # List, hence object\n",
    "    'DocsPerQuery': 'object' # List of lists, hence object\n",
    "}\n",
    "\n",
    "# Assign types to the DataFrame\n",
    "test = test.astype(column_types)\n",
    "\n",
    "# Example to add data which matches the types\n",
    "test.loc[0] = {\n",
    "    'IndexName': 'example-index',\n",
    "    'NumArticles': 100,\n",
    "    'Splitter': 'RecursiveCharacterTextSplitter',\n",
    "    'ChunkSize': 500,\n",
    "    'EmbeddingModel': 'text-embedding-3-small',\n",
    "    'Query': 'What does Socrates think about death?',\n",
    "    'QueryType': 'Simple',\n",
    "    'NumQueriesGenerated': 5,\n",
    "    'NumDocsPerQuery': 10,\n",
    "    'RerankCritique': 'R',\n",
    "    'OrigQuery': 'What does Socrates think about death?',\n",
    "    'GenQueries': ['Query 1', 'Query 2', 'Query 3'],\n",
    "    'DocsPerQuery': [[{'doc1': 'content1'}, {'doc2': 'content2'}], [{'doc3': 'content3'}], []]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b9034e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IndexName</th>\n",
       "      <th>NumArticles</th>\n",
       "      <th>Splitter</th>\n",
       "      <th>ChunkSize</th>\n",
       "      <th>EmbeddingModel</th>\n",
       "      <th>Query</th>\n",
       "      <th>QueryType</th>\n",
       "      <th>NumQueriesGenerated</th>\n",
       "      <th>NumDocsPerQuery</th>\n",
       "      <th>RerankCritique</th>\n",
       "      <th>OrigQuery</th>\n",
       "      <th>GenQueries</th>\n",
       "      <th>DocsPerQuery</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>example-index</td>\n",
       "      <td>100</td>\n",
       "      <td>RecursiveCharacterTextSplitter</td>\n",
       "      <td>500</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>What does Socrates think about death?</td>\n",
       "      <td>Simple</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>R</td>\n",
       "      <td>What does Socrates think about death?</td>\n",
       "      <td>[Query 1, Query 2, Query 3]</td>\n",
       "      <td>[[{'doc1': 'content1'}, {'doc2': 'content2'}],...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       IndexName  NumArticles                        Splitter  ChunkSize  \\\n",
       "0  example-index          100  RecursiveCharacterTextSplitter        500   \n",
       "\n",
       "           EmbeddingModel                                  Query QueryType  \\\n",
       "0  text-embedding-3-small  What does Socrates think about death?    Simple   \n",
       "\n",
       "   NumQueriesGenerated  NumDocsPerQuery RerankCritique  \\\n",
       "0                    5               10              R   \n",
       "\n",
       "                               OrigQuery                   GenQueries  \\\n",
       "0  What does Socrates think about death?  [Query 1, Query 2, Query 3]   \n",
       "\n",
       "                                        DocsPerQuery  \n",
       "0  [[{'doc1': 'content1'}, {'doc2': 'content2'}],...  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9f10988b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test.to_csv('test_records.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d383d92e",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3da93062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d5e6fd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test_records.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "2eae45b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IndexName</th>\n",
       "      <th>NumArticles</th>\n",
       "      <th>Splitter</th>\n",
       "      <th>ChunkSize</th>\n",
       "      <th>EmbeddingModel</th>\n",
       "      <th>Query</th>\n",
       "      <th>QueryType</th>\n",
       "      <th>NumQueriesGenerated</th>\n",
       "      <th>NumDocsPerQuery</th>\n",
       "      <th>RerankCritique</th>\n",
       "      <th>OrigQuery</th>\n",
       "      <th>GenQueries</th>\n",
       "      <th>DocsPerQuery</th>\n",
       "      <th>Dimensions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chunk200-text-embedding-3-small</td>\n",
       "      <td>100</td>\n",
       "      <td>RecursiveCharacterTextSplitter</td>\n",
       "      <td>200</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>Query</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Query 1', 'Query 2', 'Query 3']</td>\n",
       "      <td>[[{'doc1': 'content1'}, {'doc2': 'content2'}],...</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>naive-rag-chunk400-text-embedding-3-small-cos</td>\n",
       "      <td>100</td>\n",
       "      <td>RecursiveCharacterTextSplitter</td>\n",
       "      <td>400</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>Query</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Query 1', 'Query 2', 'Query 3']</td>\n",
       "      <td>[[{'doc1': 'content1'}, {'doc2': 'content2'}],...</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rag-test-3</td>\n",
       "      <td>100</td>\n",
       "      <td>RecursiveCharacterTextSplitter</td>\n",
       "      <td>800</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>Query</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Query 1', 'Query 2', 'Query 3']</td>\n",
       "      <td>[[{'doc1': 'content1'}, {'doc2': 'content2'}],...</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chunk-1000-text-embedding-3-small</td>\n",
       "      <td>100</td>\n",
       "      <td>RecursiveCharacterTextSplitter</td>\n",
       "      <td>1000</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>Query</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Query 1', 'Query 2', 'Query 3']</td>\n",
       "      <td>[[{'doc1': 'content1'}, {'doc2': 'content2'}],...</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       IndexName  NumArticles  \\\n",
       "0                chunk200-text-embedding-3-small          100   \n",
       "1  naive-rag-chunk400-text-embedding-3-small-cos          100   \n",
       "2                                     rag-test-3          100   \n",
       "3              chunk-1000-text-embedding-3-small          100   \n",
       "\n",
       "                         Splitter  ChunkSize          EmbeddingModel  Query  \\\n",
       "0  RecursiveCharacterTextSplitter        200  text-embedding-3-small  Query   \n",
       "1  RecursiveCharacterTextSplitter        400  text-embedding-3-small  Query   \n",
       "2  RecursiveCharacterTextSplitter        800  text-embedding-3-small  Query   \n",
       "3  RecursiveCharacterTextSplitter       1000  text-embedding-3-small  Query   \n",
       "\n",
       "   QueryType  NumQueriesGenerated  NumDocsPerQuery  RerankCritique  OrigQuery  \\\n",
       "0        NaN                  NaN              NaN             NaN        NaN   \n",
       "1        NaN                  NaN              NaN             NaN        NaN   \n",
       "2        NaN                  NaN              NaN             NaN        NaN   \n",
       "3        NaN                  NaN              NaN             NaN        NaN   \n",
       "\n",
       "                          GenQueries  \\\n",
       "0  ['Query 1', 'Query 2', 'Query 3']   \n",
       "1  ['Query 1', 'Query 2', 'Query 3']   \n",
       "2  ['Query 1', 'Query 2', 'Query 3']   \n",
       "3  ['Query 1', 'Query 2', 'Query 3']   \n",
       "\n",
       "                                        DocsPerQuery  Dimensions  \n",
       "0  [[{'doc1': 'content1'}, {'doc2': 'content2'}],...        1536  \n",
       "1  [[{'doc1': 'content1'}, {'doc2': 'content2'}],...        1536  \n",
       "2  [[{'doc1': 'content1'}, {'doc2': 'content2'}],...        1536  \n",
       "3  [[{'doc1': 'content1'}, {'doc2': 'content2'}],...        1536  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "77fd1ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('SEP.parquet')\n",
    "df['ID'] = df.index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b516831b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Url</th>\n",
       "      <th>Title</th>\n",
       "      <th>Preamble</th>\n",
       "      <th>TOC</th>\n",
       "      <th>Text</th>\n",
       "      <th>Bib</th>\n",
       "      <th>Other Resources</th>\n",
       "      <th>Related</th>\n",
       "      <th>Copyright</th>\n",
       "      <th>BibTeX</th>\n",
       "      <th>Date</th>\n",
       "      <th>Authors</th>\n",
       "      <th>BibURL</th>\n",
       "      <th>Bib_Refined</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://plato.stanford.edu/archives/spr2024/en...</td>\n",
       "      <td>18th Century German Philosophy Prior to Kant</td>\n",
       "      <td>\\n\\nKant undoubtedly casts a long shadow in th...</td>\n",
       "      <td>\\n\\n\\n1. Christian Thomasius\\n\\n1.1 Life and W...</td>\n",
       "      <td>\\n1. Christian Thomasius\\n1.1 Life and Works\\n...</td>\n",
       "      <td>\\nBibliography\\nPrimary Literature\\nBy Author\\...</td>\n",
       "      <td>\\nOther Internet Resources\\n\\nChristian-Wolff-...</td>\n",
       "      <td>\\nRelated Entries\\n\\naesthetics: German, in th...</td>\n",
       "      <td>\\n\\nCopyright © 2021 by\\n\\n\\nCorey Dyck\\n&lt;cdyc...</td>\n",
       "      <td>InCollection{sep-18thGerman-preKant,\\n\\tauthor...</td>\n",
       "      <td>2021</td>\n",
       "      <td>[{'email': 'cdyck5@uwo.ca', 'name': 'Corey Dyc...</td>\n",
       "      <td>https://plato.stanford.edu/cgi-bin/encyclopedi...</td>\n",
       "      <td>[Press, 1738, Tractatus de arte sobrie et\\nacc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://plato.stanford.edu/archives/spr2024/en...</td>\n",
       "      <td>Abduction</td>\n",
       "      <td>\\n\\nIn the philosophical literature, the term ...</td>\n",
       "      <td>\\n\\n1. Abduction: The General Idea\\n\\n1.1 Dedu...</td>\n",
       "      <td>\\n1. Abduction: The General Idea\\n\\nYou happen...</td>\n",
       "      <td>\\nBibliography\\n\\nAchinstein, P., 2001. The Bo...</td>\n",
       "      <td>\\nOther Internet Resources\\n[Please contact th...</td>\n",
       "      <td>\\nRelated Entries\\n\\nepistemology: Bayesian |\\...</td>\n",
       "      <td>\\n\\nCopyright © 2021 by\\n\\n\\nIgor Douven\\n&lt;igo...</td>\n",
       "      <td>InCollection{sep-abduction,\\n\\tauthor       =\\...</td>\n",
       "      <td>2021</td>\n",
       "      <td>[{'email': 'igor.douven@paris-sorbonne.fr', 'n...</td>\n",
       "      <td>https://plato.stanford.edu/cgi-bin/encyclopedi...</td>\n",
       "      <td>[\\nBibliography\\n\\nAchinstein, P., 2001. The B...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://plato.stanford.edu/archives/spr2024/en...</td>\n",
       "      <td>Peter Abelard</td>\n",
       "      <td>\\n\\nPeter Abelard (1079–21 April 1142) [‘Abail...</td>\n",
       "      <td>\\n\\n\\n1. Life and Works\\n\\n1.1 Life\\n1.2 Works...</td>\n",
       "      <td>\\n1. Life and Works\\n1.1 Life\\n\\nAbelard’s lif...</td>\n",
       "      <td>\\nBibliography\\nPrimary texts in Latin\\n\\nCarm...</td>\n",
       "      <td>\\nOther Internet Resources\\n\\nPierre Abelard o...</td>\n",
       "      <td>\\nRelated Entries\\n\\nAristotle, General Topics...</td>\n",
       "      <td>\\n\\nCopyright © 2022 by\\n\\n\\nPeter King\\n\\nAnd...</td>\n",
       "      <td>InCollection{sep-abelard,\\n\\tauthor       =\\t{...</td>\n",
       "      <td>2022</td>\n",
       "      <td>[{'email': None, 'name': 'Peter King'}, {'emai...</td>\n",
       "      <td>https://plato.stanford.edu/cgi-bin/encyclopedi...</td>\n",
       "      <td>[Fairweather, E. R., 1995, A Scholastic Miscel...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://plato.stanford.edu/archives/spr2024/en...</td>\n",
       "      <td>Abhidharma</td>\n",
       "      <td>\\n\\nThe first centuries after Śākyamuni Buddha...</td>\n",
       "      <td>\\n\\n1. Abhidharma: its origins and texts\\n\\n1....</td>\n",
       "      <td>\\n1. Abhidharma: its origins and texts\\n\\nThe ...</td>\n",
       "      <td>\\nBibliography\\nPrimary Sources\\n\\nThe texts a...</td>\n",
       "      <td>\\nOther Internet Resources\\n\\nAbhidharma trans...</td>\n",
       "      <td>\\nRelated Entries\\n\\natomism: 17th to 20th cen...</td>\n",
       "      <td>\\n\\nCopyright © 2022 by\\n\\n\\nNoa Ronkin\\n&lt;noa....</td>\n",
       "      <td>InCollection{sep-abhidharma,\\n\\tauthor       =...</td>\n",
       "      <td>2022</td>\n",
       "      <td>[{'email': 'noa.ronkin@wolfson.oxon.org', 'nam...</td>\n",
       "      <td>https://plato.stanford.edu/cgi-bin/encyclopedi...</td>\n",
       "      <td>[Bronkhorst, J., 2016, “Abhidharma and Indian\\...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://plato.stanford.edu/archives/spr2024/en...</td>\n",
       "      <td>Abilities</td>\n",
       "      <td>\\n\\nIn the accounts we give of one another, cl...</td>\n",
       "      <td>\\n\\n\\n1. A taxonomy\\n\\n1.1 Dispositions and ot...</td>\n",
       "      <td>\\n1. A taxonomy\\n\\nWhat is an ability? On one ...</td>\n",
       "      <td>\\nBibliography\\n\\nAlbritton, Rogers, 1985. “Fr...</td>\n",
       "      <td>\\nOther Internet Resources\\n\\nHackl, Martin, 1...</td>\n",
       "      <td>\\nRelated Entries\\n\\naction |\\n compatibilism ...</td>\n",
       "      <td>\\n\\nCopyright © 2020 by\\n\\n\\nJohn Maier\\n&lt;john...</td>\n",
       "      <td>InCollection{sep-abilities,\\n\\tauthor       =\\...</td>\n",
       "      <td>2020</td>\n",
       "      <td>[{'email': 'john@jmaier.net', 'name': 'John Ma...</td>\n",
       "      <td>https://plato.stanford.edu/cgi-bin/encyclopedi...</td>\n",
       "      <td>[Oxford University Press, 1986, 67–80.\\nOxford...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Url  \\\n",
       "0  https://plato.stanford.edu/archives/spr2024/en...   \n",
       "1  https://plato.stanford.edu/archives/spr2024/en...   \n",
       "2  https://plato.stanford.edu/archives/spr2024/en...   \n",
       "3  https://plato.stanford.edu/archives/spr2024/en...   \n",
       "4  https://plato.stanford.edu/archives/spr2024/en...   \n",
       "\n",
       "                                          Title  \\\n",
       "0  18th Century German Philosophy Prior to Kant   \n",
       "1                                     Abduction   \n",
       "2                                 Peter Abelard   \n",
       "3                                    Abhidharma   \n",
       "4                                     Abilities   \n",
       "\n",
       "                                            Preamble  \\\n",
       "0  \\n\\nKant undoubtedly casts a long shadow in th...   \n",
       "1  \\n\\nIn the philosophical literature, the term ...   \n",
       "2  \\n\\nPeter Abelard (1079–21 April 1142) [‘Abail...   \n",
       "3  \\n\\nThe first centuries after Śākyamuni Buddha...   \n",
       "4  \\n\\nIn the accounts we give of one another, cl...   \n",
       "\n",
       "                                                 TOC  \\\n",
       "0  \\n\\n\\n1. Christian Thomasius\\n\\n1.1 Life and W...   \n",
       "1  \\n\\n1. Abduction: The General Idea\\n\\n1.1 Dedu...   \n",
       "2  \\n\\n\\n1. Life and Works\\n\\n1.1 Life\\n1.2 Works...   \n",
       "3  \\n\\n1. Abhidharma: its origins and texts\\n\\n1....   \n",
       "4  \\n\\n\\n1. A taxonomy\\n\\n1.1 Dispositions and ot...   \n",
       "\n",
       "                                                Text  \\\n",
       "0  \\n1. Christian Thomasius\\n1.1 Life and Works\\n...   \n",
       "1  \\n1. Abduction: The General Idea\\n\\nYou happen...   \n",
       "2  \\n1. Life and Works\\n1.1 Life\\n\\nAbelard’s lif...   \n",
       "3  \\n1. Abhidharma: its origins and texts\\n\\nThe ...   \n",
       "4  \\n1. A taxonomy\\n\\nWhat is an ability? On one ...   \n",
       "\n",
       "                                                 Bib  \\\n",
       "0  \\nBibliography\\nPrimary Literature\\nBy Author\\...   \n",
       "1  \\nBibliography\\n\\nAchinstein, P., 2001. The Bo...   \n",
       "2  \\nBibliography\\nPrimary texts in Latin\\n\\nCarm...   \n",
       "3  \\nBibliography\\nPrimary Sources\\n\\nThe texts a...   \n",
       "4  \\nBibliography\\n\\nAlbritton, Rogers, 1985. “Fr...   \n",
       "\n",
       "                                     Other Resources  \\\n",
       "0  \\nOther Internet Resources\\n\\nChristian-Wolff-...   \n",
       "1  \\nOther Internet Resources\\n[Please contact th...   \n",
       "2  \\nOther Internet Resources\\n\\nPierre Abelard o...   \n",
       "3  \\nOther Internet Resources\\n\\nAbhidharma trans...   \n",
       "4  \\nOther Internet Resources\\n\\nHackl, Martin, 1...   \n",
       "\n",
       "                                             Related  \\\n",
       "0  \\nRelated Entries\\n\\naesthetics: German, in th...   \n",
       "1  \\nRelated Entries\\n\\nepistemology: Bayesian |\\...   \n",
       "2  \\nRelated Entries\\n\\nAristotle, General Topics...   \n",
       "3  \\nRelated Entries\\n\\natomism: 17th to 20th cen...   \n",
       "4  \\nRelated Entries\\n\\naction |\\n compatibilism ...   \n",
       "\n",
       "                                           Copyright  \\\n",
       "0  \\n\\nCopyright © 2021 by\\n\\n\\nCorey Dyck\\n<cdyc...   \n",
       "1  \\n\\nCopyright © 2021 by\\n\\n\\nIgor Douven\\n<igo...   \n",
       "2  \\n\\nCopyright © 2022 by\\n\\n\\nPeter King\\n\\nAnd...   \n",
       "3  \\n\\nCopyright © 2022 by\\n\\n\\nNoa Ronkin\\n<noa....   \n",
       "4  \\n\\nCopyright © 2020 by\\n\\n\\nJohn Maier\\n<john...   \n",
       "\n",
       "                                              BibTeX  Date  \\\n",
       "0  InCollection{sep-18thGerman-preKant,\\n\\tauthor...  2021   \n",
       "1  InCollection{sep-abduction,\\n\\tauthor       =\\...  2021   \n",
       "2  InCollection{sep-abelard,\\n\\tauthor       =\\t{...  2022   \n",
       "3  InCollection{sep-abhidharma,\\n\\tauthor       =...  2022   \n",
       "4  InCollection{sep-abilities,\\n\\tauthor       =\\...  2020   \n",
       "\n",
       "                                             Authors  \\\n",
       "0  [{'email': 'cdyck5@uwo.ca', 'name': 'Corey Dyc...   \n",
       "1  [{'email': 'igor.douven@paris-sorbonne.fr', 'n...   \n",
       "2  [{'email': None, 'name': 'Peter King'}, {'emai...   \n",
       "3  [{'email': 'noa.ronkin@wolfson.oxon.org', 'nam...   \n",
       "4  [{'email': 'john@jmaier.net', 'name': 'John Ma...   \n",
       "\n",
       "                                              BibURL  \\\n",
       "0  https://plato.stanford.edu/cgi-bin/encyclopedi...   \n",
       "1  https://plato.stanford.edu/cgi-bin/encyclopedi...   \n",
       "2  https://plato.stanford.edu/cgi-bin/encyclopedi...   \n",
       "3  https://plato.stanford.edu/cgi-bin/encyclopedi...   \n",
       "4  https://plato.stanford.edu/cgi-bin/encyclopedi...   \n",
       "\n",
       "                                         Bib_Refined  ID  \n",
       "0  [Press, 1738, Tractatus de arte sobrie et\\nacc...   1  \n",
       "1  [\\nBibliography\\n\\nAchinstein, P., 2001. The B...   2  \n",
       "2  [Fairweather, E. R., 1995, A Scholastic Miscel...   3  \n",
       "3  [Bronkhorst, J., 2016, “Abhidharma and Indian\\...   4  \n",
       "4  [Oxford University Press, 1986, 67–80.\\nOxford...   5  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "075fa006",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "43242bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fee17111",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ID'] = df.index + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d26fa8",
   "metadata": {},
   "source": [
    "# Queries to Ask\n",
    "We're going to ask 5 simple (though technical) questions. We also create 5 complex (and technical) queries -- hinging on various parts of an article, or various articles -- where the questions can be broken down to seversal subquestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647a0393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c72e386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd45814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ddaea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7fdfc4a",
   "metadata": {},
   "source": [
    "# Creating the Indexes\n",
    "Using the Indexing Pipeline. Parameters to change are mentioned in my physical notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "463578da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import re\n",
    "\n",
    "# create the length function\n",
    "def tiktoken_len(text):\n",
    "    input_string = str(tiktoken.encoding_for_model('gpt-3.5-turbo'))\n",
    "    match = re.search(r\"'(.*?)'\", input_string)\n",
    "    result = match.group(1)\n",
    "\n",
    "    tokenizer = tiktoken.get_encoding(result)\n",
    "    \n",
    "    tokens = tokenizer.encode(\n",
    "        text,\n",
    "        disallowed_special=()\n",
    "    )\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "ff67b322",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "class TextSplitter:\n",
    "    def __init__(self):\n",
    "        # Set default splitter and prompt the user for a change\n",
    "        print(\"The default splitter is RecursiveCharacterTextSplitter.\")\n",
    "        change_splitter = input(\"Do you want to use a different splitter? (type 'yes', otherwise hit enter): \").lower()\n",
    "        if change_splitter.lower().strip() == 'yes':\n",
    "            splitter_input = input(\"Available splitter: RecursiveCharacterTextSplitter. Please enter the splitter you want to use: \")\n",
    "            if splitter_input == 'RecursiveCharacterTextSplitter':\n",
    "                splitter = RecursiveCharacterTextSplitter\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported splitter type\")\n",
    "        else:\n",
    "            splitter = RecursiveCharacterTextSplitter\n",
    "        \n",
    "        # Prompt user for chunk size\n",
    "        chunk_size = int(input(\"Enter chunk size (e.g., 400): \"))\n",
    "        \n",
    "        # Prompt user for chunk overlap\n",
    "        chunk_overlap = int(input(\"Enter chunk overlap size (e.g., 20): \"))\n",
    "        \n",
    "        # Assuming 'tiktoken_len' is the length function to be used\n",
    "        length_function = tiktoken_len\n",
    "        \n",
    "        # Set default separators and offer to change them\n",
    "        default_separators = [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        print(\"Default separators are: ['\\\\n\\\\n', '\\\\n', ' ', '']\")\n",
    "        change_separators = input(\"Do you want to change the default separators? (type 'yes', otherwise hit enter): \").lower()\n",
    "        if change_separators.lower().strip() == 'yes':\n",
    "            separators = input(\"Enter separators (seprate them by space): \").split()\n",
    "        else:\n",
    "            separators = default_separators\n",
    "        \n",
    "        self.text_splitter = splitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=length_function,\n",
    "            separators=separators\n",
    "        )\n",
    "    \n",
    "    def split_text(self, text):\n",
    "        return self.text_splitter.split_text(text)\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "    #text_splitter = TextSplitter()\n",
    "    #text = text_splitter.split_text(df['Text'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "23d7dd0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Christian Thomasius\\n1.1 Life and Works\\n\\nChristian Thomasius was born on 1 January 1655 in Leipzig. He was the\\nson of Jakob Thomasius (1622–84), a well-known jurist and\\nphilosopher at the University of Leipzig who counted Leibniz among his\\nstudents. Christian (hereafter simply ‘Thomasius’)\\nmatriculated in the philosophy faculty at Leipzig in 1669, and was\\npromoted to Magister artium in 1672. As a result of his\\nfather’s lectures, particularly on Hugo Grotius’ De\\njure belli ac pacis, and his interest in Samuel Pufendorf’s\\nDe jure naturae et gentium, Thomasius took up the study of\\nlaw in Frankfurt an der Oder in 1675 and was awarded a doctorate in\\n1679. After a brief journey to Holland, Thomasius returned to Leipzig\\nwhere he worked (unhappily) as a lawyer while also holding private\\nlectures on natural jurisprudence. Thomasius attests to the\\nfundamental reorientation of his thinking effected by his reading of\\nPufendorf, and the Apologia pro se et suo libro (1674) in\\nparticular, which he credits for convincing him of the independence of\\nnatural law from theology as well as of the need to question authority\\nand resist religious intolerance (Thomasius 1688a, “Diss.\\nProem.” §§5–10; Hochstrasser 2000:\\n113–121). This new anti-authoritarian cast of mind is clearly\\nevident in a dissertation on bigamy of 1685, in which Thomasius\\ndefends the practice as consistent with natural law, and which\\nunsurprisingly led to a confrontation with a professor in the theology\\nfaculty at Leipzig. Thomasius’ pioneering decision to hold\\nlectures in German, announced (in German) in 1687, likewise provoked\\ncontroversy, as did his publication beginning in 1688 of a monthly\\njournal (the first periodical published in German), entitled the\\nMonatsgespräche, in which Thomasius commented,\\nfrequently satirically, on the local intellectual scene.\\nThomasius’ lectures and publications increasingly generated\\nconflict with the theological faculty in Leipzig, which upheld a\\nrather strict form of Lutheran orthodoxy, and while his connections\\nwith the Saxon court stood him in good stead for a time, his defence\\nof an inter-faith marriage involving a (Lutheran) Saxon count and a\\n(Calvinist) Brandenburg princess cost him his protection, and in March\\n1690 he was prohibited from publishing and holding lectures (private\\nand academic) in Electoral Saxony.\\n\\nThomasius sought refuge in Berlin, in the neighbouring state of\\nBrandenburg which was led by the Calvinist Elector Friedrich III\\n(later king Friedrich I) and had a tradition of toleration. Partly\\nthrough the support of Pufendorf himself, Thomasius was given an\\nappointment as councillor to the court, and was allowed to lecture at\\nthe Ritterakademie in Halle an der Saale, which in 1694 would become\\nthe Friedrichs-Universität, with Thomasius among the founding\\nfaculty (in law). Thomasius was soon joined by the Pietist\\norientalist, theologian and educational reformer, August Hermann\\nFrancke, who had likewise run afoul of the religious authorities in\\nLeipzig. Thomasius himself had been sympathetic with the\\nanti-scholastic and anti-authoritarian bent of the Pietists (see\\n §3.2\\n below), and had publicly defended Francke at one point in Leipzig;\\nhowever, a public break occurred when Thomasius published a criticism,\\nin 1699, of the pedagogy Francke had adopted in his famous educational\\ninstitutions in Halle. Thomasius continued to stir controversy with\\nhis lectures and publications, which frequently over-reached the\\npurview of the juristic faculty (such as his critical discussion of\\nwitchcraft trials—Beck 1969: 253–254), and breached\\ndecorum by personally attacking his theological colleagues, all of\\nwhich led to a reprimand from the Brandenburg court in 1702 and an\\norder to adhere to the boundaries between faculties. Thomasius’\\ndissertation De concubinatu of 1713, in which he contended\\nthat the use of concubines does not violate the marriage contract\\ngiven that the purpose of marriage is solely procreation, also\\ngenerated heated discussion. While Thomasius and Francke reconciled in\\n1714, Thomasius did not play a significant role in the later\\ncontroversy between Wolff and the Pietists. He died in Halle on 23\\nSeptember 1728.\\n\\nThomasius’ works cover a wide range of topics. In addition to\\nthe topical essays and dissertations already mentioned, Thomasius\\npublished major texts on natural law, including the Institutiones\\njurisprudentiae divinae (Institutions of Divine\\nJurisprudence) of 1688 and the Fundamentum iuris naturae et\\ngentium (Foundations of the Law of Nature and Nations)\\nof 1705 (for discussion of Thomasius’ contributions to natural\\nlaw theory, which will not be taken up here, see especially\\nHochstrasser 2000: ch. 4; Kühnel 2001; and Lutterbeck 2002).\\nThomasius also published on topics in theoretical and practical\\nphilosophy, especially during his time in Halle. Thomasius wrote a\\nnumber of texts on logic, such as the Introductio ad philosophiam\\nauliam (Introduction to Court Philosophy) of 1688, as\\nwell as the Einleitung zur Vernunfft-Lehre (Introduction\\nto the Doctrine of Reason) and the Ausübung der\\nVernunfft-Lehre (Application of the Doctrine of Reason)\\nboth of 1691. These were followed by a parallel exhibition of his\\nmoral philosophy in the Einleitung zur Sitten-Lehre\\n(Introduction to the Doctrine of Morals) of 1692 and the\\nAusübung der Sitten-Lehre (Application of the\\nDoctrine of Morals) of 1696, as well as an influential excursion\\ninto metaphysics in the Versuch von Wesen des Geistes\\n(Essay on the Essence of Spirit) in 1699 (second edition\\n1709).\\n1.2 Philosophy\\n\\nThomasius is decidedly not a systematic philosopher; instead, and\\nquite consistent with his mature distaste for dogmatism in all its\\nforms, he is best characterized as a conscientiously eclectic thinker\\n(M. Albrecht 1994: 398–416; Bottin & Longo 2015:\\n301–315). That said, Thomasius’ thought is unified by an\\noverarching conviction in the priority of practical life, and the\\nbelief that erudition in whatever sphere of knowledge should be\\npursued for the sake of the improvement of our will and intellect for\\nuse in our ordinary life. This is made clear, for instance, in the\\ndefinition of learnedness (“Gelahrheit”) that\\nThomasius provides at the outset of his Introduction to the\\nDoctrine of Reason:\\n\\n\\nLearnedness is knowledge through which the human being is made capable\\nof properly distinguishing the true from the false and the good from\\nthe bad […] in order that one might promote one’s own\\ntemporal and lasting welfare, and that of others, in ordinary life and\\naffairs. (Thomasius 1691a: ch. 1, §1 [2019: 18])\\n\\n\\nSignificantly, and unlike subsequent Enlightenment thinkers, Thomasius\\nis also explicit in holding that because the attainment of learnedness\\nis possible wholly through the use of the natural (rather than the\\nsupernatural) light of the mind (cf. Thomasius 1691a: ch. 1,\\n§16), it is accessible by all, regardless of gender or class (for\\nmore on this, see\\n §6.1,\\n below).\\n\\nIn light of this, the aim of logic for Thomasius is to cultivate the\\npowers of the mind, and the faculty of reason in particular, so that\\nwe are capable of discerning what the natural light of the mind\\nreveals to be true or false in any field to which it is applied. This\\ninvolves not just supplying positive guidance with respect to how\\ntruth might be recognized and attained, but also negatively\\nidentifying and dispelling prejudices that obscure the natural light.\\nThomasius’ logic, therefore, is not primarily intended to offer\\na theory of demonstration (as with scholastic logics) nor an organon\\nfor specifically scientific discovery (as with Cartesian logics or,\\nlater, with Wolff’s logic). This distinctive aim does not,\\nhowever, prevent him from weighing in on matters of traditional\\nlogical and epistemological import; for instance, he contends that\\nthought has to do with images ultimately derived from the external\\nsenses (thereby rejecting innate ideas—Thomasius 1691a: ch. 3,\\n§22), and he outlines a theory of demonstration that focuses on\\npreserving conviction in truths through their connection to\\nincontrovertible first principles.'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e92a34f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "8ab11c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "import getpass\n",
    "\n",
    "class EmbedCreator:\n",
    "    def __init__(self):\n",
    "        provider = input(\"Are you using OpenAI or Cohere embeddings? (Hit enter for opeanai and type 'cohere' otherwise.)\")\n",
    "        default_openai_api_key = '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'\n",
    "        default_cohere_api_key = 'CBzSlf1OukbDlWDnAxCLjxAdwxOmDQbYc4F5b3WG'\n",
    "        \n",
    "        if provider.lower().strip() == 'cohere':\n",
    "            print('Available: [embed-english-light-v2.0, embed-english-light-v3.0]')\n",
    "            model_name = input('Which model? ')\n",
    "            use_default_key = input(\"Use default Cohere API key? (yes/no): \").lower().strip()\n",
    "            COHERE_API_KEY = default_cohere_api_key if use_default_key == 'yes' else getpass.getpass()\n",
    "            \n",
    "            self.embed = CohereEmbeddings(\n",
    "                model=model_name,\n",
    "                apiKey=COHERE_API_KEY)\n",
    "        else:\n",
    "            provider.lower().strip() == 'openai'\n",
    "            #print('Available: [text-embedding-3-small, text-embedding-3-large]. Enter the number accrodingly:')\n",
    "            print('1. text-embedding-3-small\\n2. text-embedding-3-large')\n",
    "            \n",
    "            num = input('Which model number (type the number)? ')\n",
    "            if int(num.strip()) == 1:\n",
    "                model_name = 'text-embedding-3-small'\n",
    "            elif int(num.strip()) == 2:\n",
    "                model_name = 'text-embedding-3-large'\n",
    "            else:\n",
    "                print(\"Enter only the number.\")\n",
    "                \n",
    "            use_default_key = input(\"Change default OpenAI API key? (hit enter of no, and type 'yes' otherwise): \").lower()\n",
    "            dimension = int(input(\"Enter the dimension of the embedding model (e.g., 256, 512, 1024, 3072): \"))\n",
    "            OPENAI_API_KEY = getpass.getpass() if use_default_key == 'yes' else default_openai_api_key\n",
    "            \n",
    "            self.embed = OpenAIEmbeddings(\n",
    "                model=model_name,\n",
    "                openai_api_key=OPENAI_API_KEY,\n",
    "                dimensions = dimension)\n",
    "            \n",
    "    def embed_documents(self, texts):\n",
    "        return self.embed.embed_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "5f70a82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from tqdm.autonotebook import tqdm\n",
    "import time\n",
    "import getpass\n",
    "\n",
    "class VectorDB:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Setup API key\n",
    "        default_api_key = '25959b28-fb44-44df-9371-13b27f6f3903'  # Handle your API key securely\n",
    "        api = input(\"Do you want to change your Pinecone API key? (hit enter of no, and type 'yes' otherwise) \")\n",
    "        if api.lower().strip() == 'yes':\n",
    "            api_key = getpass.getpass()\n",
    "        else:\n",
    "            api_key = default_api_key\n",
    "\n",
    "        # Initialize Pinecone client\n",
    "        self.pc = Pinecone(api_key=api_key)\n",
    "\n",
    "        # Default cloud provider and region\n",
    "        print(\"Cloud provider default: AWS\")\n",
    "        print(\"Cloud region default: us-west-2\")\n",
    "\n",
    "        # Optionally change cloud provider or region\n",
    "        cloud_specs = input(\"Do you want to change your Pinecone cloud provider or region? (hit enter of no, and type 'yes' otherwise) \")\n",
    "        if cloud_specs.lower().strip() == 'yes':\n",
    "            cloud = input(\"What provider? \")\n",
    "            region = input(\"What region? \")\n",
    "            self.spec = ServerlessSpec(cloud=cloud, region=region)\n",
    "        else:\n",
    "            self.spec = ServerlessSpec(cloud=\"aws\", region=\"us-west-2\")\n",
    "\n",
    "    def list_indexes(self):\n",
    "        # Fetching and listing indexes\n",
    "        return self.pc.list_indexes()\n",
    "\n",
    "    def list_cloud(self, index_name=None):\n",
    "        # Use class instance index_name if not provided\n",
    "        index_name = index_name if index_name else self.index_name\n",
    "        if index_name and index_name in [index['name'] for index in self.list_indexes()]:\n",
    "            print(f\"Index '{index_name}' is configured on:\")\n",
    "            print(f\"Cloud Provider: {self.spec.cloud}\")\n",
    "            print(f\"Cloud Region: {self.spec.region}\")\n",
    "        else:\n",
    "            print(f\"Index '{index_name}' does not exist.\")\n",
    "\n",
    "    def create_index(self):\n",
    "        self.index_name = input(\"Enter the name of the index (e.g.: naive-rag-chunk400-text-embedding-3-small): \")\n",
    "        dimension = int(input(\"Enter the dimension of the index matching the embeddeing model used (e.g., 256, 512, 1024, 3072): \"))\n",
    "        default_metric = 'cosine'\n",
    "        metric = input(\"Enter the metric ('euclidean', 'cosine', 'dotproduct') -- hit enter for the default of cosine: \")\n",
    "        \n",
    "        if metric != '':\n",
    "            default_metric = metric\n",
    "        else:\n",
    "            pass\n",
    "            \n",
    "        existing_indexes = [index['name'] for index in self.list_indexes()]\n",
    "        if self.index_name not in existing_indexes:\n",
    "            print(f\"Creating index '{self.index_name}'...\")\n",
    "            self.pc.create_index(\n",
    "                self.index_name,\n",
    "                dimension=dimension,\n",
    "                metric=default_metric,\n",
    "                spec=self.spec\n",
    "            )\n",
    "            while not self.pc.describe_index(self.index_name).status['ready']:\n",
    "                self.time.sleep(1)\n",
    "            print(f\"Index '{self.index_name}' created and is now ready.\")\n",
    "        else:\n",
    "            print(f\"Index '{self.index_name}' already exists. No action taken.\")\n",
    "\n",
    "    def connect_to_index(self):\n",
    "        if self.index_name and self.index_name in [index['name'] for index in self.list_indexes()]:\n",
    "            self.index = self.pc.Index(self.index_name)\n",
    "            print(f\"Connected to index '{self.index_name}'.\")\n",
    "            return self.index\n",
    "        else:\n",
    "            raise Exception(f\"Index '{self.index_name}' does not exist.\")\n",
    "\n",
    "    def delete_index(self, index_name=None):\n",
    "        # Use class instance index_name if not provided\n",
    "        index_name = index_name if index_name else self.index_name\n",
    "        if index_name and index_name in [index['name'] for index in self.list_indexes()]:\n",
    "            self.pc.delete_index(index_name)\n",
    "            print(f\"Index '{index_name}' has been deleted.\")\n",
    "        else:\n",
    "            print(f\"Index '{index_name}' does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "72539591",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up the text splitter..\n",
      "------------------\n",
      "The default splitter is RecursiveCharacterTextSplitter.\n",
      "Do you want to use a different splitter? (type 'yes', otherwise hit enter): \n",
      "Enter chunk size (e.g., 400): 1500\n",
      "Enter chunk overlap size (e.g., 20): 20\n",
      "Default separators are: ['\\n\\n', '\\n', ' ', '']\n",
      "Do you want to change the default separators? (type 'yes', otherwise hit enter): \n",
      "\n",
      "Setting up the embedding model..\n",
      "------------------\n",
      "Are you using OpenAI or Cohere embeddings? (Hit enter for opeanai and type 'cohere' otherwise.)\n",
      "1. text-embedding-3-small\n",
      "2. text-embedding-3-large\n",
      "Which model number (type the number)? 2\n",
      "Change default OpenAI API key? (hit enter of no, and type 'yes' otherwise): \n",
      "Enter the dimension of the embedding model (e.g., 256, 512, 1024, 3072): 3072\n",
      "\n",
      "Setting up the vector database..\n",
      "------------------\n",
      "Do you want to change your Pinecone API key? (hit enter of no, and type 'yes' otherwise) \n",
      "Cloud provider default: AWS\n",
      "Cloud region default: us-west-2\n",
      "Do you want to change your Pinecone cloud provider or region? (hit enter of no, and type 'yes' otherwise) \n",
      "Enter the name of the index (e.g.: naive-rag-chunk400-text-embedding-3-small): chunk1500-text-embedding-3-large-3072\n",
      "Enter the dimension of the index matching the embeddeing model used (e.g., 256, 512, 1024, 3072): 3072\n",
      "Enter the metric ('euclidean', 'cosine', 'dotproduct') -- hit enter for the default of cosine: \n",
      "Creating index 'chunk1500-text-embedding-3-large-3072'...\n",
      "Index 'chunk1500-text-embedding-3-large-3072' created and is now ready.\n",
      "Connected to index 'chunk1500-text-embedding-3-large-3072'.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from uuid import uuid4\n",
    "import pandas as pd  # Assuming you're using a DataFrame\n",
    "\n",
    "def divide_into_three(lst):\n",
    "    n = len(lst) // 3\n",
    "    sublists = [lst[:n], lst[n:2*n], lst[2*n:]]\n",
    "    return sublists\n",
    "\n",
    "# Define a class to encapsulate the pipeline\n",
    "class TextProcessingPipeline:\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe\n",
    "        self.setup_components()\n",
    "\n",
    "    def setup_components(self):\n",
    "        print('Setting up the text splitter..')\n",
    "        print('------------------')\n",
    "        self.text_splitter = TextSplitter()\n",
    "        \n",
    "        print('\\nSetting up the embedding model..')\n",
    "        print('------------------')\n",
    "        self.embed = EmbedCreator()\n",
    "\n",
    "        print('\\nSetting up the vector database..')\n",
    "        print('------------------')\n",
    "        #self.index_name = 'xyz'\n",
    "        self.vector_db = VectorDB()\n",
    "        self.vector_db.create_index()\n",
    "        self.index = self.vector_db.connect_to_index()\n",
    "\n",
    "    def process_texts(self, batch_limit=100):\n",
    "        all_texts = []\n",
    "        all_metadatas = []\n",
    "        \n",
    "        # Process each row in the dataframe\n",
    "        for i in tqdm(range(len(self.df))):\n",
    "            metadata = {\n",
    "                'article_id': str(self.df['ID'].iloc[i]),\n",
    "                'source': self.df['Url'].iloc[i],\n",
    "                'title': self.df['Title'].iloc[i],\n",
    "                'authors': (self.df['Authors'].iloc[i]),\n",
    "                'citation': self.df['BibURL'].iloc[i],\n",
    "                'date': self.df['Date'].iloc[i]\n",
    "            }\n",
    "            \n",
    "            # Split text into chunks and create metadata for each chunk\n",
    "            record_texts = self.text_splitter.split_text(self.df['Text'].iloc[i])\n",
    "            record_metadatas = [{'chunk': j, 'text': text, **metadata} for j, text in enumerate(record_texts)]\n",
    "            \n",
    "            all_texts.extend(record_texts)\n",
    "            all_metadatas.extend(record_metadatas)\n",
    "            \n",
    "            if len(all_texts) >= batch_limit:\n",
    "                pairs = list(zip(all_texts, all_metadatas))  # Pair texts with metadatas\n",
    "                split_pairs = divide_into_three(pairs)  # Split pairs into three sublists\n",
    "                for pairs in split_pairs:\n",
    "                    sub_texts, sub_metadatas = zip(*pairs)  # Unzip pairs back into texts and metadatas\n",
    "                    self.embed_and_upsert(sub_texts, sub_metadatas)\n",
    "                all_texts = []\n",
    "                all_metadatas = []\n",
    "                \n",
    "        if all_texts:\n",
    "            pairs = list(zip(all_texts, all_metadatas))\n",
    "            split_pairs = divide_into_three(pairs)\n",
    "            for pairs in split_pairs:\n",
    "                sub_texts, sub_metadatas = zip(*pairs)\n",
    "                self.embed_and_upsert(sub_texts, sub_metadatas)\n",
    "\n",
    "    def embed_and_upsert(self, texts, metadatas):\n",
    "        ids = [str(uuid4()) for _ in range(len(texts))]\n",
    "        embeds = self.embed.embed_documents(texts)\n",
    "        self.index.upsert(vectors=zip(ids, embeds, metadatas))\n",
    "\n",
    "# Example of how to use the pipeline\n",
    "if __name__ == '__main__':\n",
    "    #df = pd.read_csv('path_to_your_data.csv')  # Load your data into a DataFrame\n",
    "    pipeline = TextProcessingPipeline(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "cd7cccaa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [02:17<00:00,  1.37s/it]\n"
     ]
    }
   ],
   "source": [
    "# For smaller text splits (e.g., 200 vs 400 or 800), use smaller batch_limit. Default is 100, I'm using 99 for splits of 200\n",
    "\n",
    "pipeline.process_texts(batch_limit=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "fa5a67b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indexes': [{'dimension': 1536,\n",
       "              'host': 'chunk-1000-text-embedding-3-small-cion06v.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'chunk-1000-text-embedding-3-small',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1024,\n",
       "              'host': 'chunk1000-text-embedding-3-large-1024-cion06v.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'chunk1000-text-embedding-3-large-1024',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 3072,\n",
       "              'host': 'chunk1500-text-embedding-3-large-3072-cion06v.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'chunk1500-text-embedding-3-large-3072',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 512,\n",
       "              'host': 'chunk400-text-embedding-3-large-512-cion06v.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'chunk400-text-embedding-3-large-512',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 512,\n",
       "              'host': 'chunk800-text-embedding-3-large-512-cion06v.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'chunk800-text-embedding-3-large-512',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1536,\n",
       "              'host': 'chunk200-text-embedding-3-small-cion06v.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'chunk200-text-embedding-3-small',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1536,\n",
       "              'host': 'rag-test-3-cion06v.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'rag-test-3',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1536,\n",
       "              'host': 'naive-rag-chunk400-text-embedding-3-small-cos-cion06v.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'naive-rag-chunk400-text-embedding-3-small-cos',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}}]}"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pinecone import Pinecone\n",
    "pc=Pinecone(api_key='25959b28-fb44-44df-9371-13b27f6f3903')\n",
    "pc.list_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "0cb3927e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pc.delete_index('chunk400-text-embedding-3-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efde8f5b",
   "metadata": {},
   "source": [
    "### Adding the index info to the test dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d238e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test = pd.read_csv('test_records.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "f106a651",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IndexName</th>\n",
       "      <th>NumArticles</th>\n",
       "      <th>Splitter</th>\n",
       "      <th>ChunkSize</th>\n",
       "      <th>EmbeddingModel</th>\n",
       "      <th>Query</th>\n",
       "      <th>QueryType</th>\n",
       "      <th>NumQueriesGenerated</th>\n",
       "      <th>NumDocsPerQuery</th>\n",
       "      <th>RerankCritique</th>\n",
       "      <th>OrigQuery</th>\n",
       "      <th>GenQueries</th>\n",
       "      <th>DocsPerQuery</th>\n",
       "      <th>Dimensions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chunk200-text-embedding-3-small</td>\n",
       "      <td>100</td>\n",
       "      <td>RecursiveCharacterTextSplitter</td>\n",
       "      <td>200</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>Query</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Query 1', 'Query 2', 'Query 3']</td>\n",
       "      <td>[[{'doc1': 'content1'}, {'doc2': 'content2'}],...</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>naive-rag-chunk400-text-embedding-3-small-cos</td>\n",
       "      <td>100</td>\n",
       "      <td>RecursiveCharacterTextSplitter</td>\n",
       "      <td>400</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>Query</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Query 1', 'Query 2', 'Query 3']</td>\n",
       "      <td>[[{'doc1': 'content1'}, {'doc2': 'content2'}],...</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rag-test-3</td>\n",
       "      <td>100</td>\n",
       "      <td>RecursiveCharacterTextSplitter</td>\n",
       "      <td>800</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>Query</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Query 1', 'Query 2', 'Query 3']</td>\n",
       "      <td>[[{'doc1': 'content1'}, {'doc2': 'content2'}],...</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chunk-1000-text-embedding-3-small</td>\n",
       "      <td>100</td>\n",
       "      <td>RecursiveCharacterTextSplitter</td>\n",
       "      <td>1000</td>\n",
       "      <td>text-embedding-3-small</td>\n",
       "      <td>Query</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Query 1', 'Query 2', 'Query 3']</td>\n",
       "      <td>[[{'doc1': 'content1'}, {'doc2': 'content2'}],...</td>\n",
       "      <td>1536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chunk400-text-embedding-3-large-512</td>\n",
       "      <td>100</td>\n",
       "      <td>RecursiveCharacterTextSplitter</td>\n",
       "      <td>400</td>\n",
       "      <td>text-embedding-3-large</td>\n",
       "      <td>Query</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Query 1, Query 2, Query 3]</td>\n",
       "      <td>[[{'doc1': 'content1'}, {'doc2': 'content2'}],...</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>chunk800-text-embedding-3-large-512</td>\n",
       "      <td>100</td>\n",
       "      <td>RecursiveCharacterTextSplitter</td>\n",
       "      <td>800</td>\n",
       "      <td>text-embedding-3-large</td>\n",
       "      <td>Query</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Query 1, Query 2, Query 3]</td>\n",
       "      <td>[[{'doc1': 'content1'}, {'doc2': 'content2'}],...</td>\n",
       "      <td>512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>chunk1000-text-embedding-3-large-1024</td>\n",
       "      <td>100</td>\n",
       "      <td>RecursiveCharacterTextSplitter</td>\n",
       "      <td>1000</td>\n",
       "      <td>text-embedding-3-large</td>\n",
       "      <td>Query</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Query 1, Query 2, Query 3]</td>\n",
       "      <td>[[{'doc1': 'content1'}, {'doc2': 'content2'}],...</td>\n",
       "      <td>1024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>chunk1500-text-embedding-3-large-3072</td>\n",
       "      <td>100</td>\n",
       "      <td>RecursiveCharacterTextSplitter</td>\n",
       "      <td>1500</td>\n",
       "      <td>text-embedding-3-large</td>\n",
       "      <td>Query</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Query 1, Query 2, Query 3]</td>\n",
       "      <td>[[{'doc1': 'content1'}, {'doc2': 'content2'}],...</td>\n",
       "      <td>3072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       IndexName  NumArticles  \\\n",
       "0                chunk200-text-embedding-3-small          100   \n",
       "1  naive-rag-chunk400-text-embedding-3-small-cos          100   \n",
       "2                                     rag-test-3          100   \n",
       "3              chunk-1000-text-embedding-3-small          100   \n",
       "4            chunk400-text-embedding-3-large-512          100   \n",
       "5            chunk800-text-embedding-3-large-512          100   \n",
       "6          chunk1000-text-embedding-3-large-1024          100   \n",
       "7          chunk1500-text-embedding-3-large-3072          100   \n",
       "\n",
       "                         Splitter  ChunkSize          EmbeddingModel  Query  \\\n",
       "0  RecursiveCharacterTextSplitter        200  text-embedding-3-small  Query   \n",
       "1  RecursiveCharacterTextSplitter        400  text-embedding-3-small  Query   \n",
       "2  RecursiveCharacterTextSplitter        800  text-embedding-3-small  Query   \n",
       "3  RecursiveCharacterTextSplitter       1000  text-embedding-3-small  Query   \n",
       "4  RecursiveCharacterTextSplitter        400  text-embedding-3-large  Query   \n",
       "5  RecursiveCharacterTextSplitter        800  text-embedding-3-large  Query   \n",
       "6  RecursiveCharacterTextSplitter       1000  text-embedding-3-large  Query   \n",
       "7  RecursiveCharacterTextSplitter       1500  text-embedding-3-large  Query   \n",
       "\n",
       "  QueryType NumQueriesGenerated NumDocsPerQuery RerankCritique OrigQuery  \\\n",
       "0       NaN                 NaN             NaN            NaN       NaN   \n",
       "1       NaN                 NaN             NaN            NaN       NaN   \n",
       "2       NaN                 NaN             NaN            NaN       NaN   \n",
       "3       NaN                 NaN             NaN            NaN       NaN   \n",
       "4       NaN                 NaN             NaN            NaN       NaN   \n",
       "5       NaN                 NaN             NaN            NaN       NaN   \n",
       "6       NaN                 NaN             NaN            NaN       NaN   \n",
       "7      None                None            None           None      None   \n",
       "\n",
       "                          GenQueries  \\\n",
       "0  ['Query 1', 'Query 2', 'Query 3']   \n",
       "1  ['Query 1', 'Query 2', 'Query 3']   \n",
       "2  ['Query 1', 'Query 2', 'Query 3']   \n",
       "3  ['Query 1', 'Query 2', 'Query 3']   \n",
       "4        [Query 1, Query 2, Query 3]   \n",
       "5        [Query 1, Query 2, Query 3]   \n",
       "6        [Query 1, Query 2, Query 3]   \n",
       "7        [Query 1, Query 2, Query 3]   \n",
       "\n",
       "                                        DocsPerQuery  Dimensions  \n",
       "0  [[{'doc1': 'content1'}, {'doc2': 'content2'}],...        1536  \n",
       "1  [[{'doc1': 'content1'}, {'doc2': 'content2'}],...        1536  \n",
       "2  [[{'doc1': 'content1'}, {'doc2': 'content2'}],...        1536  \n",
       "3  [[{'doc1': 'content1'}, {'doc2': 'content2'}],...        1536  \n",
       "4  [[{'doc1': 'content1'}, {'doc2': 'content2'}],...         512  \n",
       "5  [[{'doc1': 'content1'}, {'doc2': 'content2'}],...         512  \n",
       "6  [[{'doc1': 'content1'}, {'doc2': 'content2'}],...        1024  \n",
       "7  [[{'doc1': 'content1'}, {'doc2': 'content2'}],...        3072  "
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "46a3738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[7] = {\n",
    "    'IndexName': 'chunk1500-text-embedding-3-large-3072',  #------------------------ CHANGE\n",
    "    'NumArticles': 100,\n",
    "    'Splitter': 'RecursiveCharacterTextSplitter',\n",
    "    'ChunkSize': 1500,  #------------------------ CHANGE\n",
    "    'EmbeddingModel': 'text-embedding-3-large',  #------------------------ CHANGE\n",
    "    'Query': 'Query',\n",
    "    'QueryType': None,\n",
    "    'NumQueriesGenerated': None,\n",
    "    'NumDocsPerQuery': None,\n",
    "    'RerankCritique': None,\n",
    "    'OrigQuery': None,\n",
    "    'GenQueries': ['Query 1', 'Query 2', 'Query 3'],\n",
    "    'DocsPerQuery': [[{'doc1': 'content1'}, {'doc2': 'content2'}], [{'doc3': 'content3'}], []],\n",
    "    'Dimensions': 3072  #------------------------ CHANGE\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "a25f27db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test['EmbeddingModel'][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "f4871258",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test.drop(['Unnamed: 0.1', 'Unnamed: 0'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "b4211321",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('test_records.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00868fc",
   "metadata": {},
   "source": [
    "# Changing RAG Parameters\n",
    "We're using the advanced RAG pipepline from Notebook 7. Parameters to change are mentioned in the physical notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773dddc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de0be60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d109359",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085825a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deepanalytic)",
   "language": "python",
   "name": "deepanalytic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
