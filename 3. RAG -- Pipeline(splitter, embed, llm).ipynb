{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6593203b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b9cae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('SEP.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17962a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:100] # Change if necessary; this small fraction is for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489a7d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2abf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ID'] = df.index + 1\n",
    "df['Title'] = df['Title'].astype(str)\n",
    "df['Text'] = df['Text'].astype(str)\n",
    "df['Bib_Refined'] = df['Bib_Refined'].astype(str)\n",
    "df['Other Resources'] = df['Other Resources'].astype(str)\n",
    "df['Related'] = df['Related'].astype(str)\n",
    "df['Authors'] = df['Authors'].apply(lambda my_list: {item['name']: item['email'] for item in my_list})\n",
    "df['Authors'] = df['Authors'].apply(lambda my_list: [name + ' --- ' + (email if email is not None else 'No email provided') for name, email in my_list.items()])\n",
    "df['FINAL_TEXT'] = \"Table of Content: \" + df['TOC'] + \"\\n\\n\" + \"Text: \" + df['Text'] + \"\\n\\n\" + \"Bibliography: \" + df['Bib_Refined'] + \"\\n\\n\" + \"Other Resources: \" + df['Other Resources'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b586a779",
   "metadata": {},
   "source": [
    "# Splitting the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "37c19ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cl100k_base'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "input_string = str(tiktoken.encoding_for_model('gpt-3.5-turbo'))\n",
    "match = re.search(r\"'(.*?)'\", input_string)\n",
    "result = match.group(1)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "42edcb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import re\n",
    "\n",
    "# create the length function\n",
    "def tiktoken_len(text):\n",
    "    input_string = str(tiktoken.encoding_for_model('gpt-3.5-turbo'))\n",
    "    match = re.search(r\"'(.*?)'\", input_string)\n",
    "    result = match.group(1)\n",
    "\n",
    "    tokenizer = tiktoken.get_encoding(result)\n",
    "    \n",
    "    tokens = tokenizer.encode(\n",
    "        text,\n",
    "        disallowed_special=()\n",
    "    )\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "88c13a9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiktoken_len('hiiii ssasd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "84af3515",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "class TextSplitter:\n",
    "    def __init__(self):\n",
    "        # Set default splitter and prompt the user for a change\n",
    "        print(\"The default splitter is RecursiveCharacterTextSplitter.\")\n",
    "        change_splitter = input(\"Do you want to use a different splitter? (yes/no): \").lower()\n",
    "        if change_splitter.lower().strip() == 'yes':\n",
    "            splitter_input = input(\"Available splitter: RecursiveCharacterTextSplitter. Please enter the splitter you want to use: \")\n",
    "            if splitter_input == 'RecursiveCharacterTextSplitter':\n",
    "                splitter = RecursiveCharacterTextSplitter\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported splitter type\")\n",
    "        else:\n",
    "            splitter = RecursiveCharacterTextSplitter\n",
    "        \n",
    "        # Prompt user for chunk size\n",
    "        chunk_size = int(input(\"Enter chunk size (e.g., 400): \"))\n",
    "        \n",
    "        # Prompt user for chunk overlap\n",
    "        chunk_overlap = int(input(\"Enter chunk overlap size (e.g., 20): \"))\n",
    "        \n",
    "        # Assuming 'tiktoken_len' is the length function to be used\n",
    "        length_function = tiktoken_len\n",
    "        \n",
    "        # Set default separators and offer to change them\n",
    "        default_separators = [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        print(\"Default separators are: ['\\\\n\\\\n', '\\\\n', ' ', '']\")\n",
    "        change_separators = input(\"Do you want to change the default separators? (yes/no): \").lower()\n",
    "        if change_separators.lower().strip() == 'yes':\n",
    "            separators = input(\"Enter separators (seprate them by space): \").split()\n",
    "        else:\n",
    "            separators = default_separators\n",
    "        \n",
    "        self.text_splitter = splitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=length_function,\n",
    "            separators=separators\n",
    "        )\n",
    "    \n",
    "    def split_text(self, text):\n",
    "        return self.text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c918611",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = TextSplitter()\n",
    "text = df['FINAL_TEXT'][0]\n",
    "result = splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "02a94a89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Text: \\n1. Christian Thomasius\\n1.1 Life and Works\\n\\nChristian Thomasius was born on 1 January 1655 in Leipzig. He was the\\nson of Jakob Thomasius (1622–84), a well-known jurist and\\nphilosopher at the University of Leipzig who counted Leibniz among his\\nstudents. Christian (hereafter simply ‘Thomasius’)\\nmatriculated in the philosophy faculty at Leipzig in 1669, and was\\npromoted to Magister artium in 1672. As a result of his\\nfather’s lectures, particularly on Hugo Grotius’ De\\njure belli ac pacis, and his interest in Samuel Pufendorf’s\\nDe jure naturae et gentium, Thomasius took up the study of\\nlaw in Frankfurt an der Oder in 1675 and was awarded a doctorate in\\n1679. After a brief journey to Holland, Thomasius returned to Leipzig\\nwhere he worked (unhappily) as a lawyer while also holding private\\nlectures on natural jurisprudence. Thomasius attests to the\\nfundamental reorientation of his thinking effected by his reading of\\nPufendorf, and the Apologia pro se et suo libro (1674) in\\nparticular, which he credits for convincing him of the independence of\\nnatural law from theology as well as of the need to question authority\\nand resist religious intolerance (Thomasius 1688a, “Diss.\\nProem.” §§5–10; Hochstrasser 2000:\\n113–121). This new anti-authoritarian cast of mind is clearly\\nevident in a dissertation on bigamy of 1685, in which Thomasius\\ndefends the practice as consistent with natural law, and which\\nunsurprisingly led to a confrontation with a professor in the theology\\nfaculty at Leipzig. Thomasius’ pioneering decision to hold\\nlectures in German, announced (in German) in 1687, likewise provoked\\ncontroversy, as did his publication beginning in 1688 of a monthly\\njournal (the first periodical published in German), entitled the\\nMonatsgespräche, in which Thomasius commented,\\nfrequently satirically, on the local intellectual scene.\\nThomasius’ lectures and publications increasingly generated\\nconflict with the theological faculty in Leipzig, which upheld a\\nrather strict form of Lutheran orthodoxy, and while his connections\\nwith the Saxon court stood him in good stead for a time, his defence\\nof an inter-faith marriage involving a (Lutheran) Saxon count and a\\n(Calvinist) Brandenburg princess cost him his protection, and in March\\n1690 he was prohibited from publishing and holding lectures (private\\nand academic) in Electoral Saxony.'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fcd43130",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277\n",
      "579\n",
      "797\n",
      "783\n",
      "788\n",
      "480\n",
      "635\n",
      "782\n",
      "530\n",
      "643\n",
      "576\n",
      "526\n",
      "678\n",
      "577\n",
      "776\n",
      "787\n",
      "450\n",
      "668\n",
      "528\n",
      "496\n",
      "723\n",
      "613\n",
      "640\n",
      "298\n",
      "748\n",
      "729\n",
      "608\n",
      "678\n",
      "606\n",
      "667\n",
      "776\n",
      "768\n",
      "774\n",
      "719\n",
      "762\n",
      "772\n",
      "725\n",
      "743\n",
      "786\n",
      "519\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(result)):\n",
    "    print(tiktoken_len(result[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922278e3",
   "metadata": {},
   "source": [
    "# Creating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709f7cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "import getpass\n",
    "\n",
    "class EmbedCreator:\n",
    "    def __init__(self):\n",
    "        provider = input(\"Are you using OpenAI or Cohere embeddings? \")\n",
    "        default_openai_api_key = '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'\n",
    "        default_cohere_api_key = 'CBzSlf1OukbDlWDnAxCLjxAdwxOmDQbYc4F5b3WG'\n",
    "\n",
    "        if provider.lower().strip() == 'openai':\n",
    "            print('Available: [text-embedding-3-small, text-embedding-3-large]')\n",
    "            model_name = input('Which model? ')\n",
    "            use_default_key = input(\"Change default OpenAI API key? (yes/no): \").lower()\n",
    "            OPENAI_API_KEY = default_openai_api_key if use_default_key == 'no' else getpass.getpass()\n",
    "            \n",
    "            self.embed = OpenAIEmbeddings(\n",
    "                model=model_name,\n",
    "                openai_api_key=OPENAI_API_KEY)\n",
    "            \n",
    "        elif provider.lower().strip() == 'cohere':\n",
    "            print('Available: [embed-english-light-v2.0, embed-english-light-v3.0]')\n",
    "            model_name = input('Which model? ')\n",
    "            use_default_key = input(\"Use default Cohere API key? (yes/no): \").lower().strip()\n",
    "            COHERE_API_KEY = default_cohere_api_key if use_default_key == 'yes' else getpass.getpass()\n",
    "            \n",
    "            self.embed = CohereEmbeddings(\n",
    "                model=model_name,\n",
    "                apiKey=COHERE_API_KEY)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported Provider.\")\n",
    "            \n",
    "    def embed_documents(self, texts):\n",
    "        return self.embed.embed_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "2e081b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you using OpenAI or Cohere embeddings? openai\n",
      "Available: [text-embedding-3-small, text-embedding-3-large]\n",
      "Which model? text-embedding-3-large\n",
      "Change default OpenAI API key? (yes/no): no\n",
      "\n",
      "\n",
      "2 3072\n"
     ]
    }
   ],
   "source": [
    "# Usage example\n",
    "embed = EmbedCreator()\n",
    "\n",
    "texts = [\n",
    "    'this is the first chunk of text',\n",
    "    'then another second chunk of text is here'\n",
    "]\n",
    "\n",
    "res = embed.embed_documents(texts)\n",
    "print('\\n')\n",
    "print(len(res), len(res[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49bc0a34",
   "metadata": {},
   "source": [
    "# Creating Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "71e0d274",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import time\n",
    "import getpass\n",
    "\n",
    "class VectorDB:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Setup API key\n",
    "        default_api_key = 'e8e8297c-07af-4895-9f67-c19ece58bb3c'  # Handle your API key securely\n",
    "        api = input(\"Do you want to change your Pinecone API key? (yes/no) \")\n",
    "        if api.lower().strip() == 'yes':\n",
    "            api_key = getpass.getpass()\n",
    "        else:\n",
    "            api_key = default_api_key\n",
    "\n",
    "        # Initialize Pinecone client\n",
    "        self.pc = Pinecone(api_key=api_key)\n",
    "\n",
    "        # Default cloud provider and region\n",
    "        print(\"Cloud provider default: AWS\")\n",
    "        print(\"Cloud region default: us-west-2\")\n",
    "\n",
    "        # Optionally change cloud provider or region\n",
    "        cloud_specs = input(\"Do you want to change your Pinecone cloud provider or region? (yes/no) \")\n",
    "        if cloud_specs.lower().strip() == 'yes':\n",
    "            cloud = input(\"What provider? \")\n",
    "            region = input(\"What region? \")\n",
    "            self.spec = ServerlessSpec(cloud=cloud, region=region)\n",
    "        else:\n",
    "            self.spec = ServerlessSpec(cloud=\"aws\", region=\"us-west-2\")\n",
    "\n",
    "    def list_indexes(self):\n",
    "        # Fetching and listing indexes\n",
    "        return self.pc.list_indexes()\n",
    "\n",
    "    def list_cloud(self, index_name=None):\n",
    "        # Use class instance index_name if not provided\n",
    "        index_name = index_name if index_name else self.index_name\n",
    "        if index_name and index_name in [index['name'] for index in self.list_indexes()]:\n",
    "            print(f\"Index '{index_name}' is configured on:\")\n",
    "            print(f\"Cloud Provider: {self.spec.cloud}\")\n",
    "            print(f\"Cloud Region: {self.spec.region}\")\n",
    "        else:\n",
    "            print(f\"Index '{index_name}' does not exist.\")\n",
    "\n",
    "    def create_index(self):\n",
    "        self.index_name = input(\"Enter the name of the index (e.g.: naive-rag-chunk400-text-embedding-3-small-cos): \")\n",
    "        dimension = int(input(\"Enter the dimension of the index: \"))\n",
    "        metric = input(\"Enter the metric (e.g., 'euclidean', 'cosine'): \")\n",
    "        existing_indexes = [index['name'] for index in self.list_indexes()]\n",
    "        if self.index_name not in existing_indexes:\n",
    "            print(f\"Creating index '{self.index_name}'...\")\n",
    "            self.pc.create_index(\n",
    "                self.index_name,\n",
    "                dimension=dimension,\n",
    "                metric=metric,\n",
    "                spec=self.spec\n",
    "            )\n",
    "            while not self.pc.describe_index(self.index_name).status['ready']:\n",
    "                self.time.sleep(1)\n",
    "            print(f\"Index '{self.index_name}' created and is now ready.\")\n",
    "        else:\n",
    "            print(f\"Index '{self.index_name}' already exists. No action taken.\")\n",
    "\n",
    "    def connect_to_index(self):\n",
    "        if self.index_name and self.index_name in [index['name'] for index in self.list_indexes()]:\n",
    "            self.index = self.pc.Index(self.index_name)\n",
    "            print(f\"Connected to index '{self.index_name}'.\")\n",
    "            return self.index\n",
    "        else:\n",
    "            raise Exception(f\"Index '{self.index_name}' does not exist.\")\n",
    "\n",
    "    def delete_index(self, index_name=None):\n",
    "        # Use class instance index_name if not provided\n",
    "        index_name = index_name if index_name else self.index_name\n",
    "        if index_name and index_name in [index['name'] for index in self.list_indexes()]:\n",
    "            self.pc.delete_index(index_name)\n",
    "            print(f\"Index '{index_name}' has been deleted.\")\n",
    "        else:\n",
    "            print(f\"Index '{index_name}' does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "94bbf0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to change your Pinecone API key? (yes/no) no\n",
      "Cloud provider default: AWS\n",
      "Cloud region default: us-west-2\n",
      "Do you want to change your Pinecone cloud provider or region? (yes/no) no\n",
      "Enter the name of the index (e.g.: naive-rag-chunk400-text-embedding-3-small-cos): test-index\n",
      "Enter the dimension of the index: 158\n",
      "Enter the metric (e.g., 'euclidean', 'cosine'): euclidean\n",
      "Creating index 'test-index'...\n",
      "Index 'test-index' created and is now ready.\n",
      "{'indexes': [{'dimension': 1536,\n",
      "              'host': 'langchain-retrieval-augmentation-cion06v.svc.apw5-4e34-81fa.pinecone.io',\n",
      "              'metric': 'dotproduct',\n",
      "              'name': 'langchain-retrieval-augmentation',\n",
      "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
      "              'status': {'ready': True, 'state': 'Ready'}},\n",
      "             {'dimension': 158,\n",
      "              'host': 'test-index-cion06v.svc.apw5-4e34-81fa.pinecone.io',\n",
      "              'metric': 'euclidean',\n",
      "              'name': 'test-index',\n",
      "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
      "              'status': {'ready': True, 'state': 'Ready'}},\n",
      "             {'dimension': 1536,\n",
      "              'host': 'rag-test-3-cion06v.svc.apw5-4e34-81fa.pinecone.io',\n",
      "              'metric': 'cosine',\n",
      "              'name': 'rag-test-3',\n",
      "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
      "              'status': {'ready': True, 'state': 'Ready'}},\n",
      "             {'dimension': 1536,\n",
      "              'host': 'canopy--advanced-rag-cion06v.svc.apw5-4e34-81fa.pinecone.io',\n",
      "              'metric': 'cosine',\n",
      "              'name': 'canopy--advanced-rag',\n",
      "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
      "              'status': {'ready': True, 'state': 'Ready'}},\n",
      "             {'dimension': 1536,\n",
      "              'host': 'naive-rag-chunk400-text-embedding-3-small-cos-cion06v.svc.apw5-4e34-81fa.pinecone.io',\n",
      "              'metric': 'cosine',\n",
      "              'name': 'naive-rag-chunk400-text-embedding-3-small-cos',\n",
      "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
      "              'status': {'ready': True, 'state': 'Ready'}}]} \n",
      "\n",
      "Connected to index 'test-index'.\n",
      "Index 'test-index' is configured on:\n",
      "Cloud Provider: aws\n",
      "Cloud Region: us-west-2\n",
      "None\n",
      "Index 'test-index' has been deleted.\n",
      "Index 'test-index' does not exist.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Usage Example\n",
    "\n",
    "index_name = 'test-index'\n",
    "vector_db = VectorDB() # Create an index named 'test-index'\n",
    "vector_db.create_index()\n",
    "print(vector_db.list_indexes(),'\\n')\n",
    "index = vector_db.connect_to_index()\n",
    "print(vector_db.list_cloud(index_name))\n",
    "vector_db.delete_index(index_name)\n",
    "print(vector_db.list_cloud(index_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b55fc1",
   "metadata": {},
   "source": [
    "# Indexing Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ff64d586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up the text splitter..\n",
      "------------------\n",
      "The default splitter is RecursiveCharacterTextSplitter.\n",
      "Do you want to use a different splitter? (yes/no): no\n",
      "Enter chunk size (e.g., 400): 800\n",
      "Enter chunk overlap size (e.g., 20): 20\n",
      "Default separators are: ['\\n\\n', '\\n', ' ', '']\n",
      "Do you want to change the default separators? (yes/no): no\n",
      "------------------\n",
      "Setting up the embedding model..\n",
      "------------------\n",
      "Are you using OpenAI or Cohere embeddings? openai\n",
      "Available: [text-embedding-3-small, text-embedding-3-large]\n",
      "Which model? text-embedding-3-small\n",
      "Use default OpenAI API key? (yes/no): yes\n",
      "------------------\n",
      "Setting up the vector database..\n",
      "------------------\n",
      "Do you want to change your Pinecone API key? (yes/no) no\n",
      "Cloud provider default: AWS\n",
      "Cloud region default: us-west-2\n",
      "Do you want to change your Pinecone cloud provider or region? (yes/no) no\n",
      "Enter the name of the index: test2-april15\n",
      "Enter the dimension of the index: 1890\n",
      "Enter the metric (e.g., 'euclidean', 'cosine'): cosine\n",
      "Creating index 'test2-april15'...\n",
      "Index 'test2-april15' created and is now ready.\n",
      "Connected to index 'rag-test-auto'.\n"
     ]
    }
   ],
   "source": [
    "# A preview of the pipeline in the next cell:\n",
    "    \n",
    "from tqdm import tqdm  # Make sure to import tqdm\n",
    "from uuid import uuid4\n",
    "\n",
    "print('Setting up the text splitter..')\n",
    "print('------------------')\n",
    "\n",
    "text_splitter = TextSplitter()\n",
    "\n",
    "print('------------------')\n",
    "\n",
    "print('Setting up the embedding model..')\n",
    "print('------------------')\n",
    "embed = EmbedCreator()\n",
    "\n",
    "print('------------------')\n",
    "\n",
    "print('Setting up the vector database..')\n",
    "print('------------------')\n",
    "index_name = 'rag-test-auto'\n",
    "vector_db = VectorDB()\n",
    "vector_db.create_index()\n",
    "index = vector_db.connect_to_index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d92339fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up the text splitter..\n",
      "The default splitter is RecursiveCharacterTextSplitter.\n",
      "Do you want to use a different splitter? (yes/no): no\n",
      "Enter chunk size (e.g., 400): 800\n",
      "Enter chunk overlap size (e.g., 20): 20\n",
      "Default separators are: ['\\n\\n', '\\n', ' ', '']\n",
      "Do you want to change the default separators? (yes/no): no\n",
      "Setting up the embedding model..\n",
      "Are you using OpenAI or Cohere embeddings? openai\n",
      "Available: [text-embedding-3-small, text-embedding-3-large]\n",
      "Which model? text-embedding-3-small\n",
      "Use default OpenAI API key? (yes/no): yes\n",
      "Setting up the vector database..\n",
      "Do you want to change your Pinecone API key? (yes/no) no\n",
      "Cloud provider default: AWS\n",
      "Cloud region default: us-west-2\n",
      "Do you want to change your Pinecone cloud provider or region? (yes/no) no\n",
      "Enter the name of the index: rag-test-3\n",
      "Enter the dimension of the index: 1536\n",
      "Enter the metric (e.g., 'euclidean', 'cosine'): cosine\n",
      "Creating index 'rag-test-3'...\n",
      "Index 'rag-test-3' created and is now ready.\n",
      "Connected to index 'rag-test-3'.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from uuid import uuid4\n",
    "import pandas as pd  # Assuming you're using a DataFrame\n",
    "\n",
    "# Define a class to encapsulate the pipeline\n",
    "class TextProcessingPipeline:\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe\n",
    "        self.setup_components()\n",
    "\n",
    "    def setup_components(self):\n",
    "        print('Setting up the text splitter..')\n",
    "        print('------------------')\n",
    "        self.text_splitter = TextSplitter()\n",
    "        \n",
    "        print('\\nSetting up the embedding model..')\n",
    "        print('------------------')\n",
    "        self.embed = EmbedCreator()\n",
    "\n",
    "        print('\\nSetting up the vector database..')\n",
    "        print('------------------')\n",
    "        #self.index_name = 'xyz'\n",
    "        self.vector_db = VectorDB()\n",
    "        self.vector_db.create_index()\n",
    "        self.index = self.vector_db.connect_to_index()\n",
    "\n",
    "    def process_texts(self, batch_limit=100):\n",
    "        texts = []\n",
    "        metadatas = []\n",
    "        \n",
    "        # Process each row in the dataframe\n",
    "        for i in tqdm(range(len(self.df))):\n",
    "            metadata = {\n",
    "                'article_id': str(self.df['ID'].iloc[i]),\n",
    "                'source': self.df['Url'].iloc[i],\n",
    "                'title': self.df['Title'].iloc[i],\n",
    "                'authors': self.df['Authors'].iloc[i],\n",
    "                'citation': self.df['BibURL'].iloc[i],\n",
    "                'date': self.df['Date'].iloc[i]\n",
    "            }\n",
    "            \n",
    "            # Split text into chunks and create metadata for each chunk\n",
    "            record_texts = self.text_splitter.split_text(self.df['Text'].iloc[i])\n",
    "            record_metadatas = [{'chunk': j, 'text': text, **metadata} for j, text in enumerate(record_texts)]\n",
    "            \n",
    "            texts.extend(record_texts)\n",
    "            metadatas.extend(record_metadatas)\n",
    "            \n",
    "            # Check if batch limit is reached to process and upsert data\n",
    "            if len(texts) >= batch_limit:\n",
    "                self.embed_and_upsert(texts, metadatas)\n",
    "                texts = []\n",
    "                metadatas = []\n",
    "                \n",
    "        # Process any remaining texts and metadata after the loop\n",
    "        if texts:\n",
    "            self.embed_and_upsert(texts, metadatas)\n",
    "\n",
    "    def embed_and_upsert(self, texts, metadatas):\n",
    "        ids = [str(uuid4()) for _ in range(len(texts))]\n",
    "        embeds = self.embed.embed_documents(texts)\n",
    "        self.index.upsert(vectors=zip(ids, embeds, metadatas))\n",
    "\n",
    "# Example of how to use the pipeline\n",
    "if __name__ == '__main__':\n",
    "    #df = pd.read_csv('path_to_your_data.csv')  # Load your data into a DataFrame\n",
    "    pipeline = TextProcessingPipeline(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c6e5ff60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [01:42<00:00,  1.02s/it]\n"
     ]
    }
   ],
   "source": [
    "pipeline.process_texts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f71eb0",
   "metadata": {},
   "source": [
    "# Connecting to Index of Choice out of the Blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "832acabd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indexes': [{'dimension': 1536,\n",
       "              'host': 'langchain-retrieval-augmentation-cion06v.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'dotproduct',\n",
       "              'name': 'langchain-retrieval-augmentation',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1536,\n",
       "              'host': 'rag-test-3-cion06v.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'rag-test-3',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1536,\n",
       "              'host': 'canopy--advanced-rag-cion06v.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'canopy--advanced-rag',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}},\n",
       "             {'dimension': 1536,\n",
       "              'host': 'naive-rag-chunk400-text-embedding-3-small-cos-cion06v.svc.apw5-4e34-81fa.pinecone.io',\n",
       "              'metric': 'cosine',\n",
       "              'name': 'naive-rag-chunk400-text-embedding-3-small-cos',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}}]}"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "default_api_key = 'e8e8297c-07af-4895-9f67-c19ece58bb3c'\n",
    "pc = Pinecone(api_key=default_api_key)\n",
    "\n",
    "# List index names\n",
    "pc.list_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "c444372b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect\n",
    "index = pc.Index('naive-rag-chunk400-text-embedding-3-small-cos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "6e36af20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index.upsert(vectors=zip(ids, embeds, metadatas))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deepanalytic)",
   "language": "python",
   "name": "deepanalytic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
