{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d2959a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0867c8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('SEP.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564c5d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:100] # Change if necessary; this small fraction is for testing purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d562870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b042654",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ID'] = df.index + 1\n",
    "df['Title'] = df['Title'].astype(str)\n",
    "df['Text'] = df['Text'].astype(str)\n",
    "df['Bib_Refined'] = df['Bib_Refined'].astype(str)\n",
    "df['Other Resources'] = df['Other Resources'].astype(str)\n",
    "df['Related'] = df['Related'].astype(str)\n",
    "df['Authors'] = df['Authors'].apply(lambda my_list: {item['name']: item['email'] for item in my_list})\n",
    "df['Authors'] = df['Authors'].apply(lambda my_list: [name + ' --- ' + (email if email is not None else 'No email provided') for name, email in my_list.items()])\n",
    "df['FINAL_TEXT'] = \"Table of Content: \" + df['TOC'] + \"\\n\\n\" + \"Text: \" + df['Text'] + \"\\n\\n\" + \"Bibliography: \" + df['Bib_Refined'] + \"\\n\\n\" + \"Other Resources: \" + df['Other Resources'] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f78bd77",
   "metadata": {},
   "source": [
    "# Splitting the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "d5553b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cl100k_base'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "input_string = str(tiktoken.encoding_for_model('gpt-3.5-turbo'))\n",
    "match = re.search(r\"'(.*?)'\", input_string)\n",
    "result = match.group(1)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f7fcfcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import re\n",
    "\n",
    "# create the length function\n",
    "def tiktoken_len(text):\n",
    "    input_string = str(tiktoken.encoding_for_model('gpt-3.5-turbo'))\n",
    "    match = re.search(r\"'(.*?)'\", input_string)\n",
    "    result = match.group(1)\n",
    "\n",
    "    tokenizer = tiktoken.get_encoding(result)\n",
    "    \n",
    "    tokens = tokenizer.encode(\n",
    "        text,\n",
    "        disallowed_special=()\n",
    "    )\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "35ff1418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiktoken_len('hiiii ssasd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2073eacd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "class TextSplitter:\n",
    "    def __init__(self):\n",
    "        # Set default splitter and prompt the user for a change\n",
    "        print(\"The default splitter is RecursiveCharacterTextSplitter.\")\n",
    "        change_splitter = input(\"Do you want to use a different splitter? (yes/no): \").lower()\n",
    "        if change_splitter.lower().strip() == 'yes':\n",
    "            splitter_input = input(\"Available splitter: RecursiveCharacterTextSplitter. Please enter the splitter you want to use: \")\n",
    "            if splitter_input == 'RecursiveCharacterTextSplitter':\n",
    "                splitter = RecursiveCharacterTextSplitter\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported splitter type\")\n",
    "        else:\n",
    "            splitter = RecursiveCharacterTextSplitter\n",
    "        \n",
    "        # Prompt user for chunk size\n",
    "        chunk_size = int(input(\"Enter chunk size (e.g., 400): \"))\n",
    "        \n",
    "        # Prompt user for chunk overlap\n",
    "        chunk_overlap = int(input(\"Enter chunk overlap size (e.g., 20): \"))\n",
    "        \n",
    "        # Assuming 'tiktoken_len' is the length function to be used\n",
    "        length_function = tiktoken_len\n",
    "        \n",
    "        # Set default separators and offer to change them\n",
    "        default_separators = [\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        print(\"Default separators are: ['\\\\n\\\\n', '\\\\n', ' ', '']\")\n",
    "        change_separators = input(\"Do you want to change the default separators? (yes/no): \").lower()\n",
    "        if change_separators.lower().strip() == 'yes':\n",
    "            separators = input(\"Enter separators (seprate them by space): \").split()\n",
    "        else:\n",
    "            separators = default_separators\n",
    "        \n",
    "        self.text_splitter = splitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            length_function=length_function,\n",
    "            separators=separators\n",
    "        )\n",
    "    \n",
    "    def split_text(self, text):\n",
    "        return self.text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d9b6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = TextSplitter()\n",
    "text = df['FINAL_TEXT'][0]\n",
    "result = splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e3700c2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Text: \\n1. Christian Thomasius\\n1.1 Life and Works\\n\\nChristian Thomasius was born on 1 January 1655 in Leipzig. He was the\\nson of Jakob Thomasius (1622–84), a well-known jurist and\\nphilosopher at the University of Leipzig who counted Leibniz among his\\nstudents. Christian (hereafter simply ‘Thomasius’)\\nmatriculated in the philosophy faculty at Leipzig in 1669, and was\\npromoted to Magister artium in 1672. As a result of his\\nfather’s lectures, particularly on Hugo Grotius’ De\\njure belli ac pacis, and his interest in Samuel Pufendorf’s\\nDe jure naturae et gentium, Thomasius took up the study of\\nlaw in Frankfurt an der Oder in 1675 and was awarded a doctorate in\\n1679. After a brief journey to Holland, Thomasius returned to Leipzig\\nwhere he worked (unhappily) as a lawyer while also holding private\\nlectures on natural jurisprudence. Thomasius attests to the\\nfundamental reorientation of his thinking effected by his reading of\\nPufendorf, and the Apologia pro se et suo libro (1674) in\\nparticular, which he credits for convincing him of the independence of\\nnatural law from theology as well as of the need to question authority\\nand resist religious intolerance (Thomasius 1688a, “Diss.\\nProem.” §§5–10; Hochstrasser 2000:\\n113–121). This new anti-authoritarian cast of mind is clearly\\nevident in a dissertation on bigamy of 1685, in which Thomasius\\ndefends the practice as consistent with natural law, and which\\nunsurprisingly led to a confrontation with a professor in the theology\\nfaculty at Leipzig. Thomasius’ pioneering decision to hold\\nlectures in German, announced (in German) in 1687, likewise provoked\\ncontroversy, as did his publication beginning in 1688 of a monthly\\njournal (the first periodical published in German), entitled the\\nMonatsgespräche, in which Thomasius commented,\\nfrequently satirically, on the local intellectual scene.\\nThomasius’ lectures and publications increasingly generated\\nconflict with the theological faculty in Leipzig, which upheld a\\nrather strict form of Lutheran orthodoxy, and while his connections\\nwith the Saxon court stood him in good stead for a time, his defence\\nof an inter-faith marriage involving a (Lutheran) Saxon count and a\\n(Calvinist) Brandenburg princess cost him his protection, and in March\\n1690 he was prohibited from publishing and holding lectures (private\\nand academic) in Electoral Saxony.'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2d02c210",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277\n",
      "579\n",
      "797\n",
      "783\n",
      "788\n",
      "480\n",
      "635\n",
      "782\n",
      "530\n",
      "643\n",
      "576\n",
      "526\n",
      "678\n",
      "577\n",
      "776\n",
      "787\n",
      "450\n",
      "668\n",
      "528\n",
      "496\n",
      "723\n",
      "613\n",
      "640\n",
      "298\n",
      "748\n",
      "729\n",
      "608\n",
      "678\n",
      "606\n",
      "667\n",
      "776\n",
      "768\n",
      "774\n",
      "719\n",
      "762\n",
      "772\n",
      "725\n",
      "743\n",
      "786\n",
      "519\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(result)):\n",
    "    print(tiktoken_len(result[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59451f0",
   "metadata": {},
   "source": [
    "# Creating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb344adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "import getpass\n",
    "\n",
    "class EmbedCreator:\n",
    "    def __init__(self):\n",
    "        provider = input(\"Are you using OpenAI or Cohere embeddings? \")\n",
    "        default_openai_api_key = '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'\n",
    "        default_cohere_api_key = 'CBzSlf1OukbDlWDnAxCLjxAdwxOmDQbYc4F5b3WG'\n",
    "\n",
    "        if provider.lower().strip() == 'openai':\n",
    "            print('Available: [text-embedding-3-small, text-embedding-3-large]')\n",
    "            model_name = input('Which model? ')\n",
    "            use_default_key = input(\"Change default OpenAI API key? (yes/no): \").lower()\n",
    "            OPENAI_API_KEY = default_openai_api_key if use_default_key == 'no' else getpass.getpass()\n",
    "            \n",
    "            self.embed = OpenAIEmbeddings(\n",
    "                model=model_name,\n",
    "                openai_api_key=OPENAI_API_KEY)\n",
    "            \n",
    "        elif provider.lower().strip() == 'cohere':\n",
    "            print('Available: [embed-english-light-v2.0, embed-english-light-v3.0]')\n",
    "            model_name = input('Which model? ')\n",
    "            use_default_key = input(\"Use default Cohere API key? (yes/no): \").lower().strip()\n",
    "            COHERE_API_KEY = default_cohere_api_key if use_default_key == 'yes' else getpass.getpass()\n",
    "            \n",
    "            self.embed = CohereEmbeddings(\n",
    "                model=model_name,\n",
    "                apiKey=COHERE_API_KEY)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported Provider.\")\n",
    "            \n",
    "    def embed_documents(self, texts):\n",
    "        return self.embed.embed_documents(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "6266d0e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are you using OpenAI or Cohere embeddings? openai\n",
      "Available: [text-embedding-3-small, text-embedding-3-large]\n",
      "Which model? text-embedding-3-large\n",
      "Change default OpenAI API key? (yes/no): no\n",
      "\n",
      "\n",
      "2 3072\n"
     ]
    }
   ],
   "source": [
    "# Usage example\n",
    "embed = EmbedCreator()\n",
    "\n",
    "texts = [\n",
    "    'this is the first chunk of text',\n",
    "    'then another second chunk of text is here'\n",
    "]\n",
    "\n",
    "res = embed.embed_documents(texts)\n",
    "print('\\n')\n",
    "print(len(res), len(res[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e853dea",
   "metadata": {},
   "source": [
    "# Creating Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "7e2ebf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import time\n",
    "import getpass\n",
    "\n",
    "class VectorDB:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Setup API key\n",
    "        default_api_key = 'e8e8297c-07af-4895-9f67-c19ece58bb3c'  # Handle your API key securely\n",
    "        api = input(\"Do you want to change your Pinecone API key? (yes/no) \")\n",
    "        if api.lower().strip() == 'yes':\n",
    "            api_key = getpass.getpass()\n",
    "        else:\n",
    "            api_key = default_api_key\n",
    "\n",
    "        # Initialize Pinecone client\n",
    "        self.pc = Pinecone(api_key=api_key)\n",
    "\n",
    "        # Default cloud provider and region\n",
    "        print(\"Cloud provider default: AWS\")\n",
    "        print(\"Cloud region default: us-west-2\")\n",
    "\n",
    "        # Optionally change cloud provider or region\n",
    "        cloud_specs = input(\"Do you want to change your Pinecone cloud provider or region? (yes/no) \")\n",
    "        if cloud_specs.lower().strip() == 'yes':\n",
    "            cloud = input(\"What provider? \")\n",
    "            region = input(\"What region? \")\n",
    "            self.spec = ServerlessSpec(cloud=cloud, region=region)\n",
    "        else:\n",
    "            self.spec = ServerlessSpec(cloud=\"aws\", region=\"us-west-2\")\n",
    "\n",
    "    def list_indexes(self):\n",
    "        # Fetching and listing indexes\n",
    "        return self.pc.list_indexes()\n",
    "\n",
    "    def list_cloud(self, index_name=None):\n",
    "        # Use class instance index_name if not provided\n",
    "        index_name = index_name if index_name else self.index_name\n",
    "        if index_name and index_name in [index['name'] for index in self.list_indexes()]:\n",
    "            print(f\"Index '{index_name}' is configured on:\")\n",
    "            print(f\"Cloud Provider: {self.spec.cloud}\")\n",
    "            print(f\"Cloud Region: {self.spec.region}\")\n",
    "        else:\n",
    "            print(f\"Index '{index_name}' does not exist.\")\n",
    "\n",
    "    def create_index(self):\n",
    "        self.index_name = input(\"Enter the name of the index (e.g.: naive-rag-chunk400-text-embedding-3-small-cos): \")\n",
    "        dimension = int(input(\"Enter the dimension of the index: \"))\n",
    "        metric = input(\"Enter the metric (e.g., 'euclidean', 'cosine'): \")\n",
    "        existing_indexes = [index['name'] for index in self.list_indexes()]\n",
    "        if self.index_name not in existing_indexes:\n",
    "            print(f\"Creating index '{self.index_name}'...\")\n",
    "            self.pc.create_index(\n",
    "                self.index_name,\n",
    "                dimension=dimension,\n",
    "                metric=metric,\n",
    "                spec=self.spec\n",
    "            )\n",
    "            while not self.pc.describe_index(self.index_name).status['ready']:\n",
    "                self.time.sleep(1)\n",
    "            print(f\"Index '{self.index_name}' created and is now ready.\")\n",
    "        else:\n",
    "            print(f\"Index '{self.index_name}' already exists. No action taken.\")\n",
    "\n",
    "    def connect_to_index(self):\n",
    "        if self.index_name and self.index_name in [index['name'] for index in self.list_indexes()]:\n",
    "            self.index = self.pc.Index(self.index_name)\n",
    "            print(f\"Connected to index '{self.index_name}'.\")\n",
    "            return self.index\n",
    "        else:\n",
    "            raise Exception(f\"Index '{self.index_name}' does not exist.\")\n",
    "\n",
    "    def delete_index(self, index_name=None):\n",
    "        # Use class instance index_name if not provided\n",
    "        index_name = index_name if index_name else self.index_name\n",
    "        if index_name and index_name in [index['name'] for index in self.list_indexes()]:\n",
    "            self.pc.delete_index(index_name)\n",
    "            print(f\"Index '{index_name}' has been deleted.\")\n",
    "        else:\n",
    "            print(f\"Index '{index_name}' does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "f22e2df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to change your Pinecone API key? (yes/no) no\n",
      "Cloud provider default: AWS\n",
      "Cloud region default: us-west-2\n",
      "Do you want to change your Pinecone cloud provider or region? (yes/no) no\n",
      "Enter the name of the index (e.g.: naive-rag-chunk400-text-embedding-3-small-cos): test-index\n",
      "Enter the dimension of the index: 158\n",
      "Enter the metric (e.g., 'euclidean', 'cosine'): euclidean\n",
      "Creating index 'test-index'...\n",
      "Index 'test-index' created and is now ready.\n",
      "{'indexes': [{'dimension': 1536,\n",
      "              'host': 'langchain-retrieval-augmentation-cion06v.svc.apw5-4e34-81fa.pinecone.io',\n",
      "              'metric': 'dotproduct',\n",
      "              'name': 'langchain-retrieval-augmentation',\n",
      "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
      "              'status': {'ready': True, 'state': 'Ready'}},\n",
      "             {'dimension': 158,\n",
      "              'host': 'test-index-cion06v.svc.apw5-4e34-81fa.pinecone.io',\n",
      "              'metric': 'euclidean',\n",
      "              'name': 'test-index',\n",
      "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
      "              'status': {'ready': True, 'state': 'Ready'}},\n",
      "             {'dimension': 1536,\n",
      "              'host': 'rag-test-3-cion06v.svc.apw5-4e34-81fa.pinecone.io',\n",
      "              'metric': 'cosine',\n",
      "              'name': 'rag-test-3',\n",
      "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
      "              'status': {'ready': True, 'state': 'Ready'}},\n",
      "             {'dimension': 1536,\n",
      "              'host': 'canopy--advanced-rag-cion06v.svc.apw5-4e34-81fa.pinecone.io',\n",
      "              'metric': 'cosine',\n",
      "              'name': 'canopy--advanced-rag',\n",
      "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
      "              'status': {'ready': True, 'state': 'Ready'}},\n",
      "             {'dimension': 1536,\n",
      "              'host': 'naive-rag-chunk400-text-embedding-3-small-cos-cion06v.svc.apw5-4e34-81fa.pinecone.io',\n",
      "              'metric': 'cosine',\n",
      "              'name': 'naive-rag-chunk400-text-embedding-3-small-cos',\n",
      "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-west-2'}},\n",
      "              'status': {'ready': True, 'state': 'Ready'}}]} \n",
      "\n",
      "Connected to index 'test-index'.\n",
      "Index 'test-index' is configured on:\n",
      "Cloud Provider: aws\n",
      "Cloud Region: us-west-2\n",
      "None\n",
      "Index 'test-index' has been deleted.\n",
      "Index 'test-index' does not exist.\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Usage Example\n",
    "\n",
    "index_name = 'test-index'\n",
    "vector_db = VectorDB() # Create an index named 'test-index'\n",
    "vector_db.create_index()\n",
    "print(vector_db.list_indexes(),'\\n')\n",
    "index = vector_db.connect_to_index()\n",
    "print(vector_db.list_cloud(index_name))\n",
    "vector_db.delete_index(index_name)\n",
    "print(vector_db.list_cloud(index_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36a7d79",
   "metadata": {},
   "source": [
    "# Indexing Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "54cc9b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up the text splitter..\n",
      "------------------\n",
      "The default splitter is RecursiveCharacterTextSplitter.\n",
      "Do you want to use a different splitter? (yes/no): no\n",
      "Enter chunk size (e.g., 400): 800\n",
      "Enter chunk overlap size (e.g., 20): 20\n",
      "Default separators are: ['\\n\\n', '\\n', ' ', '']\n",
      "Do you want to change the default separators? (yes/no): no\n",
      "------------------\n",
      "Setting up the embedding model..\n",
      "------------------\n",
      "Are you using OpenAI or Cohere embeddings? openai\n",
      "Available: [text-embedding-3-small, text-embedding-3-large]\n",
      "Which model? text-embedding-3-small\n",
      "Use default OpenAI API key? (yes/no): yes\n",
      "------------------\n",
      "Setting up the vector database..\n",
      "------------------\n",
      "Do you want to change your Pinecone API key? (yes/no) no\n",
      "Cloud provider default: AWS\n",
      "Cloud region default: us-west-2\n",
      "Do you want to change your Pinecone cloud provider or region? (yes/no) no\n",
      "Enter the name of the index: test2-april15\n",
      "Enter the dimension of the index: 1890\n",
      "Enter the metric (e.g., 'euclidean', 'cosine'): cosine\n",
      "Creating index 'test2-april15'...\n",
      "Index 'test2-april15' created and is now ready.\n",
      "Connected to index 'rag-test-auto'.\n"
     ]
    }
   ],
   "source": [
    "# A preview of the pipeline in the next cell:\n",
    "    \n",
    "from tqdm import tqdm  # Make sure to import tqdm\n",
    "from uuid import uuid4\n",
    "\n",
    "print('Setting up the text splitter..')\n",
    "print('------------------')\n",
    "\n",
    "text_splitter = TextSplitter()\n",
    "\n",
    "print('------------------')\n",
    "\n",
    "print('Setting up the embedding model..')\n",
    "print('------------------')\n",
    "embed = EmbedCreator()\n",
    "\n",
    "print('------------------')\n",
    "\n",
    "print('Setting up the vector database..')\n",
    "print('------------------')\n",
    "index_name = 'rag-test-auto'\n",
    "vector_db = VectorDB()\n",
    "vector_db.create_index()\n",
    "index = vector_db.connect_to_index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "109a502e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up the text splitter..\n",
      "The default splitter is RecursiveCharacterTextSplitter.\n",
      "Do you want to use a different splitter? (yes/no): no\n",
      "Enter chunk size (e.g., 400): 800\n",
      "Enter chunk overlap size (e.g., 20): 20\n",
      "Default separators are: ['\\n\\n', '\\n', ' ', '']\n",
      "Do you want to change the default separators? (yes/no): no\n",
      "Setting up the embedding model..\n",
      "Are you using OpenAI or Cohere embeddings? openai\n",
      "Available: [text-embedding-3-small, text-embedding-3-large]\n",
      "Which model? text-embedding-3-small\n",
      "Use default OpenAI API key? (yes/no): yes\n",
      "Setting up the vector database..\n",
      "Do you want to change your Pinecone API key? (yes/no) no\n",
      "Cloud provider default: AWS\n",
      "Cloud region default: us-west-2\n",
      "Do you want to change your Pinecone cloud provider or region? (yes/no) no\n",
      "Enter the name of the index: rag-test-3\n",
      "Enter the dimension of the index: 1536\n",
      "Enter the metric (e.g., 'euclidean', 'cosine'): cosine\n",
      "Creating index 'rag-test-3'...\n",
      "Index 'rag-test-3' created and is now ready.\n",
      "Connected to index 'rag-test-3'.\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from uuid import uuid4\n",
    "import pandas as pd  # Assuming you're using a DataFrame\n",
    "\n",
    "# Define a class to encapsulate the pipeline\n",
    "class TextProcessingPipeline:\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe\n",
    "        self.setup_components()\n",
    "\n",
    "    def setup_components(self):\n",
    "        print('Setting up the text splitter..')\n",
    "        print('------------------')\n",
    "        self.text_splitter = TextSplitter()\n",
    "        \n",
    "        print('\\nSetting up the embedding model..')\n",
    "        print('------------------')\n",
    "        self.embed = EmbedCreator()\n",
    "\n",
    "        print('\\nSetting up the vector database..')\n",
    "        print('------------------')\n",
    "        #self.index_name = 'xyz'\n",
    "        self.vector_db = VectorDB()\n",
    "        self.vector_db.create_index()\n",
    "        self.index = self.vector_db.connect_to_index()\n",
    "\n",
    "    def process_texts(self, batch_limit=100):\n",
    "        texts = []\n",
    "        metadatas = []\n",
    "        \n",
    "        # Process each row in the dataframe\n",
    "        for i in tqdm(range(len(self.df))):\n",
    "            metadata = {\n",
    "                'article_id': str(self.df['ID'].iloc[i]),\n",
    "                'source': self.df['Url'].iloc[i],\n",
    "                'title': self.df['Title'].iloc[i],\n",
    "                'authors': self.df['Authors'].iloc[i],\n",
    "                'citation': self.df['BibURL'].iloc[i],\n",
    "                'date': self.df['Date'].iloc[i]\n",
    "            }\n",
    "            \n",
    "            # Split text into chunks and create metadata for each chunk\n",
    "            record_texts = self.text_splitter.split_text(self.df['Text'].iloc[i])\n",
    "            record_metadatas = [{'chunk': j, 'text': text, **metadata} for j, text in enumerate(record_texts)]\n",
    "            \n",
    "            texts.extend(record_texts)\n",
    "            metadatas.extend(record_metadatas)\n",
    "            \n",
    "            # Check if batch limit is reached to process and upsert data\n",
    "            if len(texts) >= batch_limit:\n",
    "                self.embed_and_upsert(texts, metadatas)\n",
    "                texts = []\n",
    "                metadatas = []\n",
    "                \n",
    "        # Process any remaining texts and metadata after the loop\n",
    "        if texts:\n",
    "            self.embed_and_upsert(texts, metadatas)\n",
    "\n",
    "    def embed_and_upsert(self, texts, metadatas):\n",
    "        ids = [str(uuid4()) for _ in range(len(texts))]\n",
    "        embeds = self.embed.embed_documents(texts)\n",
    "        self.index.upsert(vectors=zip(ids, embeds, metadatas))\n",
    "\n",
    "# Example of how to use the pipeline\n",
    "if __name__ == '__main__':\n",
    "    #df = pd.read_csv('path_to_your_data.csv')  # Load your data into a DataFrame\n",
    "    pipeline = TextProcessingPipeline(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "95141265",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [01:42<00:00,  1.02s/it]\n"
     ]
    }
   ],
   "source": [
    "pipeline.process_texts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa131b2",
   "metadata": {},
   "source": [
    "# Connecting to Arbitrary Index and Doing GQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "3469f36e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['langchain-retrieval-augmentation',\n",
       " 'rag-test-3',\n",
       " 'canopy--advanced-rag',\n",
       " 'naive-rag-chunk400-text-embedding-3-small-cos']"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pc.list_indexes().names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "4d8bd486",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Keys: \n",
      "Do you want to change your default Pinecone API key? (yes/no): no\n",
      "Do you want to change your default OpenAI API key? (yes/no): no\n",
      "\n",
      "Available indexes: ['langchain-retrieval-augmentation', 'rag-test-3', 'canopy--advanced-rag', 'naive-rag-chunk400-text-embedding-3-small-cos']\n",
      "Enter the name of the index you want to connect to: rag-test-3\n",
      "\n",
      "Available Embedding models: ['text-embedding-3-small', 'text-embedding-3-large', 'embed-english-light-v2.0', 'embed-english-light-v3.0']\n",
      "Enter the name of the embedding model you want to use: text-embedding-3-small\n",
      "\n",
      "Available LLM models: ['gpt-3.5-turbo', 'gpt-4', '...']\n",
      "Enter the name of the LLM model you want to use: gpt-4\n",
      "\n",
      "Enter a temperature value (0.0 to 1.0): 0\n",
      "\n",
      "Available QA Chain Types:\n",
      "1. RetrievalQA\n",
      "2. RetrievalQAWithSourcesChain\n",
      "Select the QA Chain type (enter 1 or 2): 2\n",
      "How many documents should the LLM retrieve and respond off of? 3\n",
      "How many docs to retrieve? 5\n",
      "\n",
      "Query Results: [Document(page_content=\"It is probably true that in mainstream fifth century Greek culture,\\nbelief in an afterlife of the soul was weak and unclear (Claus 1981,\\n68; Burnet 1916, 248-9). If so, it is fitting that Socrates' arguments\\nfor the immortality of the soul, most prominently in the\\nPhaedo, are offered to interlocutors who, at the outset of the\\ndiscussion, are by no means convinced of the idea. (In fact, in the\\nApology, 40c, Socrates himself is presented as being\\nnoncommittal about what happens to the soul at death, and even about\\nwhether it survives at all.) “Men find it very hard to\\nbelieve”, Cebes says at Phaedo 70a, “what you said\\nabout the soul. They think that after it has left the body it no\\nlonger exists anywhere, but that it is destroyed and dissolved on the\\nday the man dies.” This view is restated by Simmias (at 77b) as\\nthe opinion of the majority (cf. 80d); note that the view includes the\\nidea that the soul is a material thing, and is destroyed by being\\ndispersed, “like breath or smoke” (70a). Glaucon, in the\\nlast book of the Republic (608d), is taken aback by\\nSocrates' question,\\n\\n\\n“Haven't you realized that our soul is immortal and never\\ndestroyed?”\\n\\nHe looked at me with wonder and said: “No, by god, I\\nhaven't. Are you really in a position to assert that?”\", metadata={'article_id': '76', 'authors': ['Hendrik Lorenz --- hlorenz@princeton.edu'], 'chunk': 9.0, 'citation': 'https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=ancient-soul', 'date': '2009', 'source': 'https://plato.stanford.edu/archives/spr2024/entries/ancient-soul/', 'title': 'Ancient Theories of Soul'}), Document(page_content=\"What he does, in fact, conclude is that the soul is most like,\\nand most akin to, intelligible being, and that the body is most\\nlike perceptible and perishable being. To say this is plainly neither\\nto assert nor to imply (as Robinson 1995, 30, appears to think) that\\nsoul in some way or other falls short of intelligible, imperishable\\nbeing, any more than it is to assert or imply that body in some way or\\nother falls short of, or rather rises above, perceptible, perishable\\nbeing. The argument leaves it open whether soul is a perfectly\\nrespectable member of intelligible reality, the way human bodies are\\nperfectly respectable members of perceptible reality, or whether,\\nalternatively, soul has some intermediate status in between\\nintelligible and perceptible being, rising above the latter, but\\nmerely approximating to the former. Socrates does seem to take his\\nconclusion to imply, or at least strongly suggest, that it is natural\\nfor the soul either “to be altogether indissoluble, or nearly\\nso”, but, in any case, that the soul is less subject to\\ndissolution and destruction than the body, rather than, as the popular\\nview has it, more so. If this position can be established, Socrates is\\nin a position to refute the popular view that the soul, being composed\\nof ethereal stuff, is more liable to dispersion and destruction\\nthan the body.  However, as Cebes points out (88b), unless Socrates\\ncan establish that the soul is altogether exempt from destruction,\\nconfidence of survival in the face of death is misplaced. Socrates'\\nsoul may be a great deal more durable than his body, but as long as it\\nis not truly imperishable, there can be no guarantee that it will\\nsurvive Socrates' impending death. For it might have experienced any\\nnumber of incarnations already, and the current one might be its\\nlast. So Socrates launches his most elaborate and final argument for\\nthe immortality of the soul, which concludes that since life belongs\\nto soul essentially, the soul must be deathless — that is,\\nimmortal.\", metadata={'article_id': '76', 'authors': ['Hendrik Lorenz --- hlorenz@princeton.edu'], 'chunk': 11.0, 'citation': 'https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=ancient-soul', 'date': '2009', 'source': 'https://plato.stanford.edu/archives/spr2024/entries/ancient-soul/', 'title': 'Ancient Theories of Soul'}), Document(page_content=\"Moreover, apart from the question of immortality or otherwise, there\\nis the further question whether the soul, if it does have some form of\\nexistence after the person has died, “still possesses some power\\nand wisdom” (Phaedo, 70b; cf. 76c).  Answering both\\nquestions, Socrates says not only that the soul is immortal, but also\\nthat it contemplates truths after its separation from the body at the\\ntime of death. Needless to say, none of the four main lines of\\nargument that Socrates avails himself of succeeds in establishing the\\nimmortality of the soul, or in demonstrating that disembodied souls\\nenjoy lives of thought and intelligence. The arguments have been\\ndiscussed in some detail, for instance in Bostock 1986, and for our\\npurposes there is no need to state and analyze them systematically. It\\nwill suffice to comment selectively on aspects of the arguments that\\nbear directly on Plato's conception of the soul. The argument that\\nsheds most light on what Plato takes the nature of the soul to be is\\nthe affinity argument (78b-80b). This argument confronts head-on the\\nwidespread worry that the soul, at or soon after death, is destroyed\\nby being dispersed.  It begins by distinguishing between two kinds of\\nthings: on the one hand, things that are perceptible, composed of\\nparts, and subject to dissolution and destruction; on the other hand,\\nthings that are not perceptible, but intelligible (grasped by\\nthought), not composed of parts, and exempt from dissolution and\\ndestruction. These two categories are obviously mutually exclusive. It\\nis not clear whether or not they are meant to be exhaustive.\\nMoreover, the category of imperishable, intelligible being is\\nexemplified, but not, it seems, exhausted, by Platonic forms such as\\nequality, beauty and the like (contra Bostock 1986, 118).\\nIntelligible being evidently includes what Socrates calls the divine,\\nwhose nature it is to rule and to lead (80a), and there is no\\nindication that the forms exhaust the divine, or even include the\\ndivine, so understood.  Thus the argument leaves room for the idea\\nthat souls are not forms, but are nevertheless intelligible, partless\\nand imperishable (contra Robinson 1995, 29). In fact, in\\nframing the argument in the way he does Plato furnishes the conceptual\\nframework needed for saying that body and soul differ in kind, the one\\nbeing perceptible and perishable, the other being intelligible and\\nexempt from destruction. However, the argument does not support such a\\nstrong conclusion, and Socrates is aware of this.\", metadata={'article_id': '76', 'authors': ['Hendrik Lorenz --- hlorenz@princeton.edu'], 'chunk': 10.0, 'citation': 'https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=ancient-soul', 'date': '2009', 'source': 'https://plato.stanford.edu/archives/spr2024/entries/ancient-soul/', 'title': 'Ancient Theories of Soul'}), Document(page_content='That debate has had to confront the fact that Socrates did not\\nactually disobey his own death sentence with which his trial\\nconcluded: when the time came, he drank the poisonous hemlock as\\nprescribed by the jury. Before that moment, Plato imagines Socrates\\nbeing visited in prison by his friend Crito (in a dialogue which bears\\nthe latter’s name), and urged to escape for the sake of his\\nfriends and family, a practice which was frequently tolerated in\\nAthens so long as the escapee fled into exile. Socrates is not\\npersuaded by Crito’s arguments. He begins his examination of\\nthem by recalling principles to which he and Crito had in the past\\nagreed, including the principle that it is better to suffer injustice\\nthan to commit it (Cri. 47a–50a). He then goes on to\\nventriloquize a series of speeches against escape, which he ascribes\\nto the “Laws of\\n Athens”.[6]\\n These speeches articulate a set of special connections between\\nSocrates and the Laws of Athens which, depending on one’s\\nreading, either flesh out the principle that it is better to suffer\\ninjustice than to do it (by dramatizing reasons for which it would\\nunjust for Socrates to escape), or else stand in tension with it by\\ninvoking absolutist grounds with implications more extensive than such\\na principle would authorize (Harte 1999). On any reading, it is\\nimportant to bear in mind that Socrates is choosing to obey a jury\\nverdict that has commanded him to suffer what is arguably an\\ninjustice, but not to commit one, so that one of his fundamental\\nethical principles here as elsewhere in Plato (that it is better to\\nsuffer an injustice than to commit one) is at least compatible with\\nhis acceptance of the jury’s sentence.\\n\\nThe “Laws of Athens” appeal to a kind of social contract\\nmade between themselves and Socrates. The contract is unequal: the\\n“Laws” compare themselves to parents and slaveowners, and\\nSocrates to child and slave. Obedience to them is owed, they claim,\\nbecause the “Laws” have provided the whole basis for\\nSocrates’ education and life in the city, a city in which he has\\nnotably chosen to remain, never traveling abroad except on military\\nservice. But the “Laws” also speak of the opportunity they\\nafford to Socrates to “persuade or obey” them (51b;\\n51e–52a). The meaning of this clause and its relevance to civil\\ndisobedience is again much debated (Kraut 1984 remains a landmark).\\nNevertheless, the image of Socrates tried, convicted, and made to die\\n(by his own hand) at the city’s command has come to be a vivid\\nand powerful symbol of tension in the relationship between political\\nphilosophy and political authority.\\n3.3 The Defense of Justice in the Republic', metadata={'article_id': '75', 'authors': ['Melissa Lane --- mslane@princeton.edu'], 'chunk': 8.0, 'citation': 'https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=ancient-political', 'date': '2023', 'source': 'https://plato.stanford.edu/archives/spr2024/entries/ancient-political/', 'title': 'Ancient Political Philosophy'}), Document(page_content=\"It hardly needs pointing out, then, that the soul, as Plato conceives\\nof it in the Phaedo, is crucially characterized by cognitive\\nand intellectual features: it is something that reasons, more or less\\nwell depending on the extent to which it is disturbed or distracted by\\nthe body and the senses; something that regulates and controls the\\nbody and its desires and affections, “especially if it is a wise\\nsoul” (94b), presumably in a way that involves, and renders\\neffective, judgments about what it is best to do, and how it is best\\nto behave; and something that has, as the kind of adornment that is\\ntruly appropriate to it, virtues such as temperance, justice and\\ncourage (114e f.). However, it should be clear that the soul, as it is\\nconceived of here, is not simply the mind, as we conceive of it. It is\\nboth broader and narrower than that. It is broader in that Plato\\nevidently retains the traditional idea of soul as distinguishing the\\nanimate from the inanimate. Two of the four main lines of argument for\\nthe immortality of the soul rely not on cognitive or indeed\\nspecifically psychological features of the soul, but simply on the\\nfamiliar connection between soul and life. According to the cyclical\\nargument (70c-72d), being alive in general is preceded by, just as it\\nprecedes, being dead.  Socrates takes this to show that a creature's\\ndeath involves the continued existence of the soul in question, which\\npersists through a period of separation from body, and then returns to\\nanimate another body in a change which is the counterpart of the\\nprevious change, dying. According to the last line of argument that\\nSocrates offers in the Phaedo, the soul is immortal because it\\nhas life essentially, the way fire has heat essentially. It is plain\\nthat both of these arguments apply to the souls of all living things,\\nincluding plants (cf. 70d, 71d). And in the final argument, Socrates\\nexplicitly appeals to the idea that it is the soul that animates the\\nbody of a living thing (105c): \\n\\nWhat is it that, when present in a body, makes it living? — A\\nsoul.\", metadata={'article_id': '76', 'authors': ['Hendrik Lorenz --- hlorenz@princeton.edu'], 'chunk': 13.0, 'citation': 'https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=ancient-soul', 'date': '2009', 'source': 'https://plato.stanford.edu/archives/spr2024/entries/ancient-soul/', 'title': 'Ancient Theories of Soul'})]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: {'question': '\\nWhat does Socrates think about death?\\n', 'answer': 'Socrates believed in the immortality of the soul. He argued that the soul is most like, and most akin to, intelligible being, and that the body is most like perceptible and perishable being. He also believed that the soul contemplates truths after its separation from the body at the time of death. However, he acknowledged that his arguments did not conclusively establish the immortality of the soul or demonstrate that disembodied souls enjoy lives of thought and intelligence.\\n', 'sources': 'https://plato.stanford.edu/archives/spr2024/entries/ancient-soul/'}\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "from pinecone import Pinecone\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "\n",
    "class Naive_RAG:\n",
    "    def __init__(self):\n",
    "        self.pinecone_api_key = self.setup_pinecone_api_key()\n",
    "        self.openai_api_key = self.setup_openai_api_key()\n",
    "        self.pc = self.setup_pinecone()\n",
    "        self.index = self.connect_to_index()\n",
    "        self.embed = self.load_embedding_model()\n",
    "        self.vectorstore = PineconeVectorStore(self.index, self.embed)\n",
    "        self.llm = self.setup_llm()\n",
    "        self.qa = self.setup_qa()\n",
    "\n",
    "    def setup_pinecone_api_key(self):\n",
    "        print('API Keys: ')\n",
    "        change_key = input(\"Do you want to change your default Pinecone API key? (yes/no): \")\n",
    "        if change_key.lower() == 'yes':\n",
    "            return getpass.getpass(\"Enter your Pinecone API key: \")\n",
    "        else:\n",
    "            return 'c5a1e31b-332e-4ac4-9f53-2042f80c7bfe'\n",
    "        \n",
    "    def setup_openai_api_key(self):\n",
    "        change_key = input(\"Do you want to change your default OpenAI API key? (yes/no): \")\n",
    "        if change_key.lower() == 'yes':\n",
    "            return getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "        else:\n",
    "            return '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'\n",
    "\n",
    "    def setup_pinecone(self):\n",
    "        return Pinecone(api_key=self.pinecone_api_key)\n",
    "\n",
    "    def connect_to_index(self):\n",
    "        index_names = self.pc.list_indexes().names()\n",
    "        print(\"\\nAvailable indexes:\", index_names)\n",
    "        index_name = input(\"Enter the name of the index you want to connect to: \")\n",
    "        return self.pc.Index(index_name)\n",
    "\n",
    "    def load_embedding_model(self):\n",
    "        model_names = ['text-embedding-3-small', 'text-embedding-3-large', 'embed-english-light-v2.0', 'embed-english-light-v3.0']\n",
    "        print(\"\\nAvailable Embedding models:\", model_names)\n",
    "        model_name = input(\"Enter the name of the embedding model you want to use: \")\n",
    "        return OpenAIEmbeddings(\n",
    "            model=model_name,\n",
    "            openai_api_key=self.openai_api_key\n",
    "        )\n",
    "\n",
    "    def setup_llm(self):\n",
    "        llm_models = ['gpt-3.5-turbo', 'gpt-4', '...']\n",
    "        print(\"\\nAvailable LLM models:\", llm_models)\n",
    "        llm_model = input(\"Enter the name of the LLM model you want to use: \")\n",
    "        temp = float(input(\"\\nEnter a temperature value (0.0 to 1.0): \"))\n",
    "        return ChatOpenAI(\n",
    "            openai_api_key=self.openai_api_key,\n",
    "            model_name=llm_model,\n",
    "            temperature=temp\n",
    "        )\n",
    "\n",
    "    def setup_qa(self):\n",
    "        # Provide a choice between RetrievalQA and RetrievalQAWithSourcesChain\n",
    "        print(\"\\nAvailable QA Chain Types:\")\n",
    "        print(\"1. RetrievalQA\")\n",
    "        print(\"2. RetrievalQAWithSourcesChain\")\n",
    "        choice = input(\"Select the QA Chain type (enter 1 or 2): \")\n",
    "        n=int(input(\"How many documents should the LLM retrieve and respond off of? \"))\n",
    "        \n",
    "        if choice == '1':\n",
    "            return RetrievalQA.from_chain_type(\n",
    "                llm=self.llm,\n",
    "                chain_type=\"stuff\",  # Adjust as necessary based on available chain types\n",
    "                retriever=self.vectorstore.as_retriever(search_kwargs={'k': n})\n",
    "            )\n",
    "        elif choice == '2':\n",
    "            return RetrievalQAWithSourcesChain.from_chain_type(\n",
    "                llm=self.llm,\n",
    "                chain_type=\"stuff\",\n",
    "                retriever=self.vectorstore.as_retriever(search_kwargs={'k': n}),\n",
    "            )\n",
    "        else:\n",
    "            print(\"Invalid selection, defaulting to RetrievalQA.\")\n",
    "            return RetrievalQA.from_chain_type(\n",
    "                llm=self.llm,\n",
    "                chain_type=\"stuff\",\n",
    "                retriever=self.vectorstore.as_retriever(search_kwargs={'k': 3})\n",
    "            )\n",
    "\n",
    "    # This only retrieves docs, nothing genrative about it. Default to 5\n",
    "    def query(self, query_text):\n",
    "        k=input(\"How many docs to retrieve? \")\n",
    "        results = self.vectorstore.similarity_search(\n",
    "            query_text,\n",
    "            k=int(k)\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    def invoke_qa(self, question):\n",
    "        answer = self.qa.invoke(question)\n",
    "        return answer\n",
    "\n",
    "# Example usage\n",
    "if __name__ == '__main__':\n",
    "    krs = Naive_RAG()\n",
    "    query_result = krs.query(\"What does Socrates think about death?\")\n",
    "    print(\"\\nQuery Results:\", query_result)\n",
    "    print('\\n')\n",
    "    answer = krs.invoke_qa(\"\\nWhat does Socrates think about death?\\n\")\n",
    "    print(\"Answer:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "c18e8b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available QA Chain Types:\n",
      "1. RetrievalQA\n",
      "2. RetrievalQAWithSourcesChain\n",
      "Select the QA Chain type (enter 1 or 2): 2\n",
      "How many documents should the LLM retrieve and respond off of? 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What does Scorates think about death?',\n",
       " 'answer': \"Socrates' arguments for the immortality of the soul are offered to interlocutors who are not convinced of the idea. The majority believe that the soul is destroyed and dissolved on the day the man dies. Socrates himself is presented as being noncommittal about what happens to the soul at death. Seneca believed that suicide is a supreme mark of freedom.\\n\",\n",
       " 'sources': 'https://plato.stanford.edu/archives/spr2024/entries/afterlife/, https://plato.stanford.edu/archives/spr2024/entries/ancient-soul/, https://plato.stanford.edu/archives/spr2024/entries/ancient-political/'}"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "krs.setup_qa().invoke(\"What does Scorates think about death?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "9f598749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '\\nWhat does Socrates think about death?\\n',\n",
       " 'answer': \"Socrates' arguments for the immortality of the soul are offered to interlocutors who are not convinced of the idea. In the Apology, Socrates himself is presented as being noncommittal about what happens to the soul at death. \\n\",\n",
       " 'sources': 'https://plato.stanford.edu/archives/spr2024/entries/ancient-soul/'}"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "4195a39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What does Socrates think about death?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(answer['question'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535363fd",
   "metadata": {},
   "source": [
    "# Adding a Reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7080de38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa80b7ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d874ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (deepanalytic)",
   "language": "python",
   "name": "deepanalytic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
